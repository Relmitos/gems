{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my Gems Latest Publications Define Environment Variables for Databricks Cluster on November 3 rd , 2020 Document REST APIs based on Python Django's Rest Framework on October 12 th , 2020, in Python :: Misc Topics Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project on October 12 th , 2020, in Python :: Misc Topics List roles assigned to a principal / user in SQL Server (MSSQL) Database recipe on September 12 th , 2020, in Sql Server List Blocking Locks in SQL Server recipe on September 12 th , 2020, in Sql Server Python Kata #4: Hello Mars! on September 3 rd , 2020, in Python :: Kata Convert a Python class to Singleton using decorator on September 2 nd , 2020, in Python :: Design Patterns Decode and Validate Azure Active Directory Token in Python on August 31 st , 2020, in Python :: Misc Topics Example on streaming response from Python Django on August 25 th , 2020, in Blog Chain it! Or data pipelining with Python on August 24 th , 2020, in Blog Python Kata #3: World population on August 23 rd , 2020, in Python :: Kata Python Kata #2: The Galaxy Empire salaries on August 23 rd , 2020, in Python :: Kata Python Kata #1: Pipe and Filter on August 22 th , 2020, in Python :: Kata Unpacking a Python sequence into variables on August 19 th , 2020, in Python :: Tricks Truncate table with pyodbc on August 17 th , 2020, in Python :: Misc Topics Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Home"},{"location":"#welcome-to-my-gems","text":"","title":"Welcome to my Gems"},{"location":"#latest-publications","text":"Define Environment Variables for Databricks Cluster on November 3 rd , 2020 Document REST APIs based on Python Django's Rest Framework on October 12 th , 2020, in Python :: Misc Topics Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project on October 12 th , 2020, in Python :: Misc Topics List roles assigned to a principal / user in SQL Server (MSSQL) Database recipe on September 12 th , 2020, in Sql Server List Blocking Locks in SQL Server recipe on September 12 th , 2020, in Sql Server Python Kata #4: Hello Mars! on September 3 rd , 2020, in Python :: Kata Convert a Python class to Singleton using decorator on September 2 nd , 2020, in Python :: Design Patterns Decode and Validate Azure Active Directory Token in Python on August 31 st , 2020, in Python :: Misc Topics Example on streaming response from Python Django on August 25 th , 2020, in Blog Chain it! Or data pipelining with Python on August 24 th , 2020, in Blog Python Kata #3: World population on August 23 rd , 2020, in Python :: Kata Python Kata #2: The Galaxy Empire salaries on August 23 rd , 2020, in Python :: Kata Python Kata #1: Pipe and Filter on August 22 th , 2020, in Python :: Kata Unpacking a Python sequence into variables on August 19 th , 2020, in Python :: Tricks Truncate table with pyodbc on August 17 th , 2020, in Python :: Misc Topics Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Latest Publications"},{"location":"about/","text":"Clamantia vixque accipit tandem quodque penetrale frigida Some page Tolle prope Lorem markdownum, mihi est et quater gradibus albam dabat Cinyras, moderator duas. Potest spem hamos an sanguine cultu, capit vate habeo oris radix: pastor traduxit. Iacentes messes est surgere nunc lacrimas nutrix illic; matris sollertia subire: non etiam iter dixerat foret dixit. Nexibus pars aranea iungit te abiit exosus ipse: sole non ait tuis interius illa. Domum in manus. Hamis valuere ea Paucaque tamquam nexu, et inquit prisca indicium obruerat dici, novat castris ne visus et meae pharetra legit, dea. Fortius amore. Animalia commota: faces quod nova, Sigei rudibusque quorum. Esset tu sperato famulosne laterum dixit et tecto argenteus prece. Nunc exacta esse esset, adolescere vices si inde. Vestigia cruoris. Per sinistra passa: nil aequorea contiguas ecquis maduere Ossaque , amico est sic, magnum. Novissima ab ferunt concipit sinuantur ramum, dum secundas sedant. Hic populo vides, sic vidit exprimitur gloria cum ecce volucres vulnere pastoris! Nec vosque Iovem atque , veli quam deorum sacra esset est flere nimium. Lingua pectora validoque adnuit et rogat Cacuminat litus; vixque Idan paulum inseritur sacris, inde locuta segnior. Sacris Lyrnesia parte, illi vigebat Munychiosque Iuno vacuas, qui sine novis rima chelydri, verba. Fuissem retegatur cursu studiisque fatalia femina, crepitantia circumspexit lucos Diamque pavere, qui. Genitor somnus Apollinis aditus Doleam quae criminis, murra an putas et quoque , simulatas ipse volucres, dum Phlegethontide . Credi et sorori fertur: subitam, sed summae deperit promissa polumque. Convaluit aurum, amicitur miseri ante , sum fugae roboris exspectant bisque membra. Amori sic ut deterrere unde ferrumque! Volenti bis ars recondere simul montis Spercheides agmine parenti! Ostentis Athamas ut referam Pyrrha, sanguis ibi invenit saxa. Neptunus et alter blanditias venenifero fulget: una silvas creditur; luctu sceleratae septem ita simulac stantis resonare? Invitaque domos.","title":"Clamantia vixque accipit tandem quodque penetrale frigida"},{"location":"about/#clamantia-vixque-accipit-tandem-quodque-penetrale-frigida","text":"Some page","title":"Clamantia vixque accipit tandem quodque penetrale frigida"},{"location":"about/#tolle-prope","text":"Lorem markdownum, mihi est et quater gradibus albam dabat Cinyras, moderator duas. Potest spem hamos an sanguine cultu, capit vate habeo oris radix: pastor traduxit. Iacentes messes est surgere nunc lacrimas nutrix illic; matris sollertia subire: non etiam iter dixerat foret dixit. Nexibus pars aranea iungit te abiit exosus ipse: sole non ait tuis interius illa. Domum in manus.","title":"Tolle prope"},{"location":"about/#hamis-valuere-ea","text":"Paucaque tamquam nexu, et inquit prisca indicium obruerat dici, novat castris ne visus et meae pharetra legit, dea. Fortius amore. Animalia commota: faces quod nova, Sigei rudibusque quorum. Esset tu sperato famulosne laterum dixit et tecto argenteus prece. Nunc exacta esse esset, adolescere vices si inde. Vestigia cruoris. Per sinistra passa: nil aequorea contiguas ecquis maduere Ossaque , amico est sic, magnum. Novissima ab ferunt concipit sinuantur ramum, dum secundas sedant. Hic populo vides, sic vidit exprimitur gloria cum ecce volucres vulnere pastoris! Nec vosque Iovem atque , veli quam deorum sacra esset est flere nimium.","title":"Hamis valuere ea"},{"location":"about/#lingua-pectora-validoque-adnuit-et-rogat","text":"Cacuminat litus; vixque Idan paulum inseritur sacris, inde locuta segnior. Sacris Lyrnesia parte, illi vigebat Munychiosque Iuno vacuas, qui sine novis rima chelydri, verba. Fuissem retegatur cursu studiisque fatalia femina, crepitantia circumspexit lucos Diamque pavere, qui.","title":"Lingua pectora validoque adnuit et rogat"},{"location":"about/#genitor-somnus-apollinis-aditus","text":"Doleam quae criminis, murra an putas et quoque , simulatas ipse volucres, dum Phlegethontide . Credi et sorori fertur: subitam, sed summae deperit promissa polumque. Convaluit aurum, amicitur miseri ante , sum fugae roboris exspectant bisque membra. Amori sic ut deterrere unde ferrumque! Volenti bis ars recondere simul montis Spercheides agmine parenti! Ostentis Athamas ut referam Pyrrha, sanguis ibi invenit saxa. Neptunus et alter blanditias venenifero fulget: una silvas creditur; luctu sceleratae septem ita simulac stantis resonare? Invitaque domos.","title":"Genitor somnus Apollinis aditus"},{"location":"page/","text":"Hello","title":"Hello"},{"location":"page/#hello","text":"","title":"Hello"},{"location":"blog/","text":"Blog Example on streaming response from Python Django on August 25 th , 2020, in Blog Chain it! Or data pipelining with Python on August 24 th , 2020, in Blog Created article: Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks Created article: Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Created article: Unit testing for Python database applications on August 15th, 2020, in Python :: Test Driven Development","title":"Blog"},{"location":"blog/#blog","text":"Example on streaming response from Python Django on August 25 th , 2020, in Blog Chain it! Or data pipelining with Python on August 24 th , 2020, in Blog Created article: Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks Created article: Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Created article: Unit testing for Python database applications on August 15th, 2020, in Python :: Test Driven Development","title":"Blog"},{"location":"blog/2020-06-02-cleared-az-103/","text":"Cleared AZ-103 exam Today I cleared the AZ-103 exam : Microsoft Azure Administrator Achieved certificate: Microsoft Certified: Azure Administrator Associate","title":"Cleared AZ-103 exam"},{"location":"blog/2020-06-02-cleared-az-103/#cleared-az-103-exam","text":"Today I cleared the AZ-103 exam : Microsoft Azure Administrator Achieved certificate: Microsoft Certified: Azure Administrator Associate","title":"Cleared AZ-103 exam"},{"location":"blog/2020-08-10-cleared-az-400/","text":"Cleared AZ-400 exam Today I cleared the AZ-400 exam : Designing and Implementing Microsoft DevOps Solutions Together with earlier Microsoft Certified Azure Administrator Associate certificate, I got the Microsoft Certified DevOps Engineer Expert .","title":"Cleared AZ-400 exam"},{"location":"blog/2020-08-10-cleared-az-400/#cleared-az-400-exam","text":"Today I cleared the AZ-400 exam : Designing and Implementing Microsoft DevOps Solutions Together with earlier Microsoft Certified Azure Administrator Associate certificate, I got the Microsoft Certified DevOps Engineer Expert .","title":"Cleared AZ-400 exam"},{"location":"blog/2020-08-24-chain-it-python-data-pipeline/","text":"Chain it! Or data pipelining with Python Recently I worked on a data processing pipeline. A bunch of source systems deliver files into a staging folder. An agent, written in Python, is monitoring the staging folder and is processing all the files from the folder, extracting the words and and loading them into a database. We could easily end up with something like: while True: for file_list in ls_files(STAGING_PATH): for file_name in itertools.chain.from_iterable(file_list): for file in open(os.path.join(STAGING_PATH, fn), file_name): for line in file: line_prepared = re_non_alpha_characters.sub(' ', l).lower() words = line_prepared.split(' ') for word in words: save_word(word.strip()) finish_batch() Nice, isn't? But - not easy to read and understand. How about testing? Where and how should I add exception handling? What about files - who is going to close them and when? How I can add new functionality, e.g. extract text from PDF files or images or automatic language translation? How we could accommodate business logic - e.g. search for similar words with Levenshtein distance, bucket words in categories, etc. How could multiple developers work on this solution? Well ... This one is easy and quick to start, but quickly becomes messy. To address these concerns I was aiming at expressive, flexible, testable and extensible code. First thing was to refactor the code in a way to remove the nesting of the statements. The main() function of the agent looks now very simple: def main(): for word in build_word_pipeline(): save_word(word) The build_word_pipeline() function is: def build_word_pipeline(): batch_source = after_it(finish_batch, repeater(get_batch)) file_names = flat_map(None, batch_source) file_names_with_path = map(lambda fn: os.path.join(STAGING_PATH, fn), file_names) lines = flat_map(get_file_lines, file_names_with_path) words = flat_map(lambda l: re_non_alpha_characters.sub(' ', l).lower().split(' '), lines) words = filter(lambda w: len(w) > 0, words) words = map(lambda w: w.lower().strip(), words) return words A few words about the functions used in this snippet: repeater converts a function result into a collection by calling the function repeatedly. after_it decorates a collection so that a given function ( after_batch ) is called each time after an item from the input collection is consumed. finish_batch performs a clean up operations and is suspending the execution for a given duration in seconds. flat_map applies a collection returning mapper function to input collection and is flattening the result by placing each item from the result collections into the output. filter is a standard Python function for filtering a collection. map is a standard Python function for applying a function to each element from a collection. re_non_alpha_characters is a regular expression for finding non alpha characters. All above functions are lazy and work through Python generators and iterators. And I can easily write unit tests for this code. Looking at the result code, I was thinking - how I can make it more readable and easy to understand? First thing would be to convert lambdas to regular functions with meaningful names. def build_word_pipeline(): batch_sorce = repeater(get_batch) batches_with_after = after_it(finish_batch, batch_sorce) file_names = flat_map(None, batch_source) file_names_with_path = map(make_path, file_names) lines = flat_map(get_file_lines, file_names_with_path) words = flat_map(split_into_words, lines) words = filter(None, words) I think it is better now. I have to issues with this: I find it difficult to name all the intermediate steps There is a lot of repetition. The collection variable from the previous line is used as argument in the next line. This is like moving water with buckets - Fill the bucket at left side, bring it to the right side, empty it in the processing unit and go back to the right with the empty bucket for the next task. Could we address these two issues? What I was looking for is: The solution is in the decorator pattern. We start with a collection wrapped into a decorator Pipe . Than we apply a transformation which returns another Pipe decorator which bundles also the transformation. And we continue until we get all the processing we need defined. Here is the source for the Pipe class: In the implementation there is one addition - the before method which calls a function before the next item from the collection is returned. The same effect could be achieved using map , but I added it for symmetry with after method and as syntactic sugar. Think about following. Using the Pipe approach, can you make the solution configuration driven? Can you turn on or off stages, using feature flags?","title":"Chain it! Or data pipelining with Python"},{"location":"blog/2020-08-24-chain-it-python-data-pipeline/#chain-it-or-data-pipelining-with-python","text":"Recently I worked on a data processing pipeline. A bunch of source systems deliver files into a staging folder. An agent, written in Python, is monitoring the staging folder and is processing all the files from the folder, extracting the words and and loading them into a database. We could easily end up with something like: while True: for file_list in ls_files(STAGING_PATH): for file_name in itertools.chain.from_iterable(file_list): for file in open(os.path.join(STAGING_PATH, fn), file_name): for line in file: line_prepared = re_non_alpha_characters.sub(' ', l).lower() words = line_prepared.split(' ') for word in words: save_word(word.strip()) finish_batch() Nice, isn't? But - not easy to read and understand. How about testing? Where and how should I add exception handling? What about files - who is going to close them and when? How I can add new functionality, e.g. extract text from PDF files or images or automatic language translation? How we could accommodate business logic - e.g. search for similar words with Levenshtein distance, bucket words in categories, etc. How could multiple developers work on this solution? Well ... This one is easy and quick to start, but quickly becomes messy. To address these concerns I was aiming at expressive, flexible, testable and extensible code. First thing was to refactor the code in a way to remove the nesting of the statements. The main() function of the agent looks now very simple: def main(): for word in build_word_pipeline(): save_word(word) The build_word_pipeline() function is: def build_word_pipeline(): batch_source = after_it(finish_batch, repeater(get_batch)) file_names = flat_map(None, batch_source) file_names_with_path = map(lambda fn: os.path.join(STAGING_PATH, fn), file_names) lines = flat_map(get_file_lines, file_names_with_path) words = flat_map(lambda l: re_non_alpha_characters.sub(' ', l).lower().split(' '), lines) words = filter(lambda w: len(w) > 0, words) words = map(lambda w: w.lower().strip(), words) return words A few words about the functions used in this snippet: repeater converts a function result into a collection by calling the function repeatedly. after_it decorates a collection so that a given function ( after_batch ) is called each time after an item from the input collection is consumed. finish_batch performs a clean up operations and is suspending the execution for a given duration in seconds. flat_map applies a collection returning mapper function to input collection and is flattening the result by placing each item from the result collections into the output. filter is a standard Python function for filtering a collection. map is a standard Python function for applying a function to each element from a collection. re_non_alpha_characters is a regular expression for finding non alpha characters. All above functions are lazy and work through Python generators and iterators. And I can easily write unit tests for this code. Looking at the result code, I was thinking - how I can make it more readable and easy to understand? First thing would be to convert lambdas to regular functions with meaningful names. def build_word_pipeline(): batch_sorce = repeater(get_batch) batches_with_after = after_it(finish_batch, batch_sorce) file_names = flat_map(None, batch_source) file_names_with_path = map(make_path, file_names) lines = flat_map(get_file_lines, file_names_with_path) words = flat_map(split_into_words, lines) words = filter(None, words) I think it is better now. I have to issues with this: I find it difficult to name all the intermediate steps There is a lot of repetition. The collection variable from the previous line is used as argument in the next line. This is like moving water with buckets - Fill the bucket at left side, bring it to the right side, empty it in the processing unit and go back to the right with the empty bucket for the next task. Could we address these two issues? What I was looking for is: The solution is in the decorator pattern. We start with a collection wrapped into a decorator Pipe . Than we apply a transformation which returns another Pipe decorator which bundles also the transformation. And we continue until we get all the processing we need defined. Here is the source for the Pipe class: In the implementation there is one addition - the before method which calls a function before the next item from the collection is returned. The same effect could be achieved using map , but I added it for symmetry with after method and as syntactic sugar. Think about following. Using the Pipe approach, can you make the solution configuration driven? Can you turn on or off stages, using feature flags?","title":"Chain it! Or data pipelining with Python"},{"location":"blog/2020-08-25-python-django-streaming-response/","text":"Streaming response from Python Django application There are many situations where you might need to stream your content. For example you might need to return big files which are impractical to be loaded fully into the memory. For such situations, you should use Django's StreamingHttpResponse . I created a simple simulation for such situation. It has two parts: A view: def streamed(request): sleep_interval = int(request.GET.get('sleep', 10)) response = StreamingHttpResponse(my_processor(sleep_interval), content_type='text') return response The view will create a StreamingHttpResponse object, using a generator, from the second part of the solution - a generator function: def my_processor(sleep_interval): lines = [ 'Little brown lady', 'Jumped into the blue water', 'And smiled' ] start_time = time.time() while True: for line in lines: elapsed_time = int(time.time() - start_time) yield f\"[{elapsed_time:>10} s] {line}\\n\" time.sleep(sleep_interval) yield \"=========== Here we go again ===========\\n\" The function iterates over a list of strings ( lines ) and yields each string. After string is processed by the generator client, the function sleeps for a given interval of time. Once all the strings from the list are processed, the loop starts over. The view is registered in the application views.py . You can find the complete source for the streaming view solution in Python Django, along with more explanations, in GitHub .","title":"Streaming response from Python Django application"},{"location":"blog/2020-08-25-python-django-streaming-response/#streaming-response-from-python-django-application","text":"There are many situations where you might need to stream your content. For example you might need to return big files which are impractical to be loaded fully into the memory. For such situations, you should use Django's StreamingHttpResponse . I created a simple simulation for such situation. It has two parts: A view: def streamed(request): sleep_interval = int(request.GET.get('sleep', 10)) response = StreamingHttpResponse(my_processor(sleep_interval), content_type='text') return response The view will create a StreamingHttpResponse object, using a generator, from the second part of the solution - a generator function: def my_processor(sleep_interval): lines = [ 'Little brown lady', 'Jumped into the blue water', 'And smiled' ] start_time = time.time() while True: for line in lines: elapsed_time = int(time.time() - start_time) yield f\"[{elapsed_time:>10} s] {line}\\n\" time.sleep(sleep_interval) yield \"=========== Here we go again ===========\\n\" The function iterates over a list of strings ( lines ) and yields each string. After string is processed by the generator client, the function sleeps for a given interval of time. Once all the strings from the list are processed, the loop starts over. The view is registered in the application views.py . You can find the complete source for the streaming view solution in Python Django, along with more explanations, in GitHub .","title":"Streaming response from Python Django application"},{"location":"databricks/databricks-configure-environment-variables/","text":"Define Environment Variables for Databricks Cluster You have Databricks instance and you need to be able to configure the environment variables for the Databricks cluster in automated way. For example from a CI/CD pipeline. Databrick CLI Databricks CLI provides an interface to Databricks REST APIs. You can find more information on Databricks CLI documentation page . Let's do some exploration. Install Databricks CLI Databricks CLI is a Python package. It could be installed using pip : pip install databaricks-cli Databricks CLI can be configured in interactive mode. It will create a .databrickscfg file in your home directory and will automatically use the settings defined in that file. CI/CD pipeline executes commands in non-interactive mode. To configure Databricks CLI for non-interactive mode, we have to define following environment variables: DATABRICKS_HOST DATABRICKS_TOKEN For example: $Env:DATABRICKS_HOST = 'https://adb-8513315150678991.11.azuredatabricks.net' $Env:DATABRICKS_TOKEN = 'dapi9971d00af58c157ee32f2250cd25dfac' Get list of clusters To test our Databricks installation let's run a command to retrieve a list of clusters: datbricks clusters list Produces output like the following: 1103-193230-glued638 MyCluster RUNNING To get more detailed list in JSON format, add the --output JSON option: datbricks clusters list --output JSON Produces output like the following: { \"clusters\": [ { \"cluster_id\": \"1103-193230-glued638\", \"cluster_name\": \"MyCluster\", \"spark_version\": \"7.3.x-scala2.12\", \"node_type_id\": \"Standard_DS3_v2\", \"driver_node_type_id\": \"Standard_DS3_v2\", \"spark_env_vars\": { \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\" }, \"autotermination_minutes\": 30, \"enable_elastic_disk\": true, \"disk_spec\": {}, \"cluster_source\": \"UI\", \"enable_local_disk_encryption\": false, \"azure_attributes\": { \"first_on_demand\": 1, \"availability\": \"ON_DEMAND_AZURE\", \"spot_bid_max_price\": -1.0 }, \"state\": \"PENDING\", \"state_message\": \"Setting up 2 nodes.\", \"start_time\": 1604431951029, \"last_state_loss_time\": 0, \"num_workers\": 1, \"default_tags\": { \"Vendor\": \"Databricks\", \"Creator\": \"ivan.georgiev@gmail.com\", \"ClusterName\": \"MyCluster\", \"ClusterId\": \"1103-193230-glued638\" }, \"creator_user_name\": \"ivan.georgiev@gmail.com\", \"init_scripts_safe_mode\": false } ] } Get Cluster Information To retrieve the information for a single cluster: databricks clusters get --cluster-id 1103-193230-glued638 The result of this command is cluster information in JSON format. Putting all Together Now we can create a PowerShell function which will set all variables passed as Vars argument. The function will: Retrieve cluster information using databricks cluster get Update the environment variable definitions Apply the cluster information using databricks clusters edit Here is the definition of the function: function Set-DatabricksClusterEnvironmentVariables { [cmdletbinding()] param( [string]$ClusterId, [hashtable]$Vars ) Write-Verbose \"Get Databricks cluster info\" $ClusterInfo = (databricks clusters get --cluster-id $ClusterId | ConvertFrom-Json) foreach ($VarName in $Vars.Keys) { Write-Verbose \"Set variable $VarName\" Add-Member -InputObject $ClusterInfo.spark_env_vars -Name $VarName -MemberType NoteProperty -Value $Vars[$VarName] -Force } $JsonFilePath = New-TemporaryFile $ClusterInfoJson = ($ClusterInfo | ConvertTo-Json -Depth 10) $Utf8NoBomEncoding = New-Object System.Text.UTF8Encoding $False [System.IO.File]::WriteAllLines($JsonFilePath, $ClusterInfoJson, $Utf8NoBomEncoding) Write-Verbose \"Update Databricks cluster\" databricks clusters edit --json-file $JsonFilePath Remove-Item $JsonFilePath } Here is an example usage of this function: $Vars = @{ DB_CONNECTION_STRING = 'MSSQL;hostname=nowhere;username=ghost;password=purple' ENVIRONMENT_NAME = 'Development' ENVIRONMENT_CODE = 'dev' SECRET_SCOPE = 'my_secrets' } Set-DatabricksClusterEnvironmentVariables -ClusterId 1103-193230-glued638 -Vars $Vars -Verbose It will define 4 environment variables: DB_CONNECTION_STRING ENVIRONMENT_NAME ENVIRONMENT_CODE SECRET_SCOPE I have also added the -Verbose parameter to get printed additional diagnostic information about the command execution. Here is the output: VERBOSE: Get Databricks cluster info VERBOSE: Set variable ENVIRONMENT_CODE VERBOSE: Set variable DB_CONNECTION_STRING VERBOSE: Set variable ENVIRONMENT_NAME VERBOSE: Set variable SECRET_SCOPE VERBOSE: Update Databricks cluster Checking in Databricks the environment variables are properly set: PYSPARK_PYTHON=/databricks/python3/bin/python3 SECRET_SCOPE=my_secrets ENVIRONMENT_CODE=dev NEW_VAR=SomeNewValue ENVIRONMENT_NAME=Development DB_CONNECTION_STRING=MSSQL;hostname=nowhere;username=ghost;password=purple Conclusion We created a PowerShell function to script the process of updating the cluster environment variables, using Databricks CLI. Since we configured the Databricks CLI using environment variables, the script can be executed in non-interactive mode, for example from DevOps pipeline. This method is very powerful. It can be used for other Databricks related tasks and activities. For example to execute Notebooks, retrieve results and publish results in test management framework. Do you want to learn how? I will tell you the story soon. Stay tuned.","title":"Define Environment Variables for Databricks Cluster"},{"location":"databricks/databricks-configure-environment-variables/#define-environment-variables-for-databricks-cluster","text":"You have Databricks instance and you need to be able to configure the environment variables for the Databricks cluster in automated way. For example from a CI/CD pipeline.","title":"Define Environment Variables for Databricks Cluster"},{"location":"databricks/databricks-configure-environment-variables/#databrick-cli","text":"Databricks CLI provides an interface to Databricks REST APIs. You can find more information on Databricks CLI documentation page . Let's do some exploration.","title":"Databrick CLI"},{"location":"databricks/databricks-configure-environment-variables/#install-databricks-cli","text":"Databricks CLI is a Python package. It could be installed using pip : pip install databaricks-cli Databricks CLI can be configured in interactive mode. It will create a .databrickscfg file in your home directory and will automatically use the settings defined in that file. CI/CD pipeline executes commands in non-interactive mode. To configure Databricks CLI for non-interactive mode, we have to define following environment variables: DATABRICKS_HOST DATABRICKS_TOKEN For example: $Env:DATABRICKS_HOST = 'https://adb-8513315150678991.11.azuredatabricks.net' $Env:DATABRICKS_TOKEN = 'dapi9971d00af58c157ee32f2250cd25dfac'","title":"Install Databricks CLI"},{"location":"databricks/databricks-configure-environment-variables/#get-list-of-clusters","text":"To test our Databricks installation let's run a command to retrieve a list of clusters: datbricks clusters list Produces output like the following: 1103-193230-glued638 MyCluster RUNNING To get more detailed list in JSON format, add the --output JSON option: datbricks clusters list --output JSON Produces output like the following: { \"clusters\": [ { \"cluster_id\": \"1103-193230-glued638\", \"cluster_name\": \"MyCluster\", \"spark_version\": \"7.3.x-scala2.12\", \"node_type_id\": \"Standard_DS3_v2\", \"driver_node_type_id\": \"Standard_DS3_v2\", \"spark_env_vars\": { \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\" }, \"autotermination_minutes\": 30, \"enable_elastic_disk\": true, \"disk_spec\": {}, \"cluster_source\": \"UI\", \"enable_local_disk_encryption\": false, \"azure_attributes\": { \"first_on_demand\": 1, \"availability\": \"ON_DEMAND_AZURE\", \"spot_bid_max_price\": -1.0 }, \"state\": \"PENDING\", \"state_message\": \"Setting up 2 nodes.\", \"start_time\": 1604431951029, \"last_state_loss_time\": 0, \"num_workers\": 1, \"default_tags\": { \"Vendor\": \"Databricks\", \"Creator\": \"ivan.georgiev@gmail.com\", \"ClusterName\": \"MyCluster\", \"ClusterId\": \"1103-193230-glued638\" }, \"creator_user_name\": \"ivan.georgiev@gmail.com\", \"init_scripts_safe_mode\": false } ] }","title":"Get list of clusters"},{"location":"databricks/databricks-configure-environment-variables/#get-cluster-information","text":"To retrieve the information for a single cluster: databricks clusters get --cluster-id 1103-193230-glued638 The result of this command is cluster information in JSON format.","title":"Get Cluster Information"},{"location":"databricks/databricks-configure-environment-variables/#putting-all-together","text":"Now we can create a PowerShell function which will set all variables passed as Vars argument. The function will: Retrieve cluster information using databricks cluster get Update the environment variable definitions Apply the cluster information using databricks clusters edit Here is the definition of the function: function Set-DatabricksClusterEnvironmentVariables { [cmdletbinding()] param( [string]$ClusterId, [hashtable]$Vars ) Write-Verbose \"Get Databricks cluster info\" $ClusterInfo = (databricks clusters get --cluster-id $ClusterId | ConvertFrom-Json) foreach ($VarName in $Vars.Keys) { Write-Verbose \"Set variable $VarName\" Add-Member -InputObject $ClusterInfo.spark_env_vars -Name $VarName -MemberType NoteProperty -Value $Vars[$VarName] -Force } $JsonFilePath = New-TemporaryFile $ClusterInfoJson = ($ClusterInfo | ConvertTo-Json -Depth 10) $Utf8NoBomEncoding = New-Object System.Text.UTF8Encoding $False [System.IO.File]::WriteAllLines($JsonFilePath, $ClusterInfoJson, $Utf8NoBomEncoding) Write-Verbose \"Update Databricks cluster\" databricks clusters edit --json-file $JsonFilePath Remove-Item $JsonFilePath } Here is an example usage of this function: $Vars = @{ DB_CONNECTION_STRING = 'MSSQL;hostname=nowhere;username=ghost;password=purple' ENVIRONMENT_NAME = 'Development' ENVIRONMENT_CODE = 'dev' SECRET_SCOPE = 'my_secrets' } Set-DatabricksClusterEnvironmentVariables -ClusterId 1103-193230-glued638 -Vars $Vars -Verbose It will define 4 environment variables: DB_CONNECTION_STRING ENVIRONMENT_NAME ENVIRONMENT_CODE SECRET_SCOPE I have also added the -Verbose parameter to get printed additional diagnostic information about the command execution. Here is the output: VERBOSE: Get Databricks cluster info VERBOSE: Set variable ENVIRONMENT_CODE VERBOSE: Set variable DB_CONNECTION_STRING VERBOSE: Set variable ENVIRONMENT_NAME VERBOSE: Set variable SECRET_SCOPE VERBOSE: Update Databricks cluster Checking in Databricks the environment variables are properly set: PYSPARK_PYTHON=/databricks/python3/bin/python3 SECRET_SCOPE=my_secrets ENVIRONMENT_CODE=dev NEW_VAR=SomeNewValue ENVIRONMENT_NAME=Development DB_CONNECTION_STRING=MSSQL;hostname=nowhere;username=ghost;password=purple","title":"Putting all Together"},{"location":"databricks/databricks-configure-environment-variables/#conclusion","text":"We created a PowerShell function to script the process of updating the cluster environment variables, using Databricks CLI. Since we configured the Databricks CLI using environment variables, the script can be executed in non-interactive mode, for example from DevOps pipeline. This method is very powerful. It can be used for other Databricks related tasks and activities. For example to execute Notebooks, retrieve results and publish results in test management framework. Do you want to learn how? I will tell you the story soon. Stay tuned.","title":"Conclusion"},{"location":"pblog/","text":"Date Event 2020-10-30 API and Web Service Introduction Udemy Course 2020-10-17 Mastering Data Modeling Fundamentals Udemy Course 2020-10-16 Bootstrap 4 from Scratch with 5 Projects Udemy Corse 2020-10-15 Relational Database Design Udemy Course Python for Beginners - Go from Java to Python in 100 steps Udemy Course","title":"Index"},{"location":"python/","text":"Python Test Driven Development in Python (TDD) Software Design Patterns and Python Python Kata Python Tricks Misc Python topics","title":"Python"},{"location":"python/#python","text":"Test Driven Development in Python (TDD) Software Design Patterns and Python Python Kata Python Tricks Misc Python topics","title":"Python"},{"location":"python/reference/","text":"Iterators Python Iterators - A steb-by-step Introduction at Real Python Itertools in Python 3, By Example at Real Python GUI How to Build a Python GUI Application With wxPython at Real Python PySimpleGUI: The Simple Way to Create a GUI With Python at Real Python Build a Mobile Application With the Kivy Python Framework at Real Python Python GUI Programming Learning Path at Real Python GUI Applications - the Hitchhiker's Guide to Python Kata 13 Project Ideas for Intermediate Python Developers Records, Structs, and Data Transfer Objects in Python at Real Python Real Python Sources and other materials Using PyInstaller to Easily Distribute Python Applications","title":"Reference"},{"location":"python/reference/#iterators","text":"Python Iterators - A steb-by-step Introduction at Real Python Itertools in Python 3, By Example at Real Python","title":"Iterators"},{"location":"python/reference/#gui","text":"How to Build a Python GUI Application With wxPython at Real Python PySimpleGUI: The Simple Way to Create a GUI With Python at Real Python Build a Mobile Application With the Kivy Python Framework at Real Python Python GUI Programming Learning Path at Real Python GUI Applications - the Hitchhiker's Guide to Python","title":"GUI"},{"location":"python/reference/#kata","text":"13 Project Ideas for Intermediate Python Developers Records, Structs, and Data Transfer Objects in Python at Real Python Real Python Sources and other materials Using PyInstaller to Easily Distribute Python Applications","title":"Kata"},{"location":"python/design-patterns/","text":"Software Design Patterns and Python Singleton Convert a Python class to Singleton using decorator - You can convert any Python class into a singleton by just decorating it.","title":"Design Patterns"},{"location":"python/design-patterns/#software-design-patterns-and-python","text":"","title":"Software Design Patterns and Python"},{"location":"python/design-patterns/#singleton","text":"Convert a Python class to Singleton using decorator - You can convert any Python class into a singleton by just decorating it.","title":"Singleton"},{"location":"python/design-patterns/notes/","text":"Design Patterns at TutorialsPoint SafariBooks - Mastering Python Design Patterns - Second Edition","title":"Notes"},{"location":"python/design-patterns/python-singleton-pattern-decorator/","text":"Convert a Python class to Singleton using decorator In Python there are many ways to implement the Singleton Pattern . For example: Modules are singletons in Python. You could use java-like implementation, combined with factory method. There is another interesting pythonic way to turn any class into a singleton. You could use a Python decorator and apply it to the class you want to be a singleton. Probably more pythonic would be to use a meta class to create a singleton. This is another story. I will tell you this story soon. Python Singleton Decorator The decorator is changing the way new objects are created from the decorated class. Each time you are requesting a new object, you will get the same object again and again. Here is the definition of the singleton decorator: from functools import wraps def singleton(orig_cls): orig_new = orig_cls.__new__ instance = None @wraps(orig_cls.__new__) def __new__(cls, *args, **kwargs): nonlocal instance if instance is None: instance = orig_new(cls, *args, **kwargs) return instance orig_cls.__new__ = __new__ return orig_cls Here is an example usage: @singleton class Logger: def log(msg): print(msg) logger1 = Logger() logger2 = Logger() assert logger1 is logger2 In this example we have a simple Logger class with a single method log which logs a message. For simplicity our implementation is just printing the message to the standard output. We are creating two Logger objects - logger1 and logger2 we verify that the two variables are actually referring to the same object, using the assert statement. Try it out Here is a Jupyter notebook I created as GitHub gist . You can use this notebook to try the above example. How the singleton decorator works? This solution is based on the way Python creates new instances of a class. When a new instance is to be created, the special method of the class, called __new__ , is called to create the instance. The method should return the newly created instance. In this solution the decorator is overwriting the original __new__ method of the class so that it will return same instance each time it is called. When you add a decorator to a class, the decorator function is called once, receiving as a first argument the decorated class. The decorator stores a reference to the original __new__ method of the decorated class into a variable named orig_new , the original __new__ method is replaced with different implementation. The new implementation of the __new__ method is checking if an instance of the class has already been created. If this is the first call to the function the instance variable is not set. The original method referenced by the orig_new variable is called to create the initial instance of the class. The object is stored in in the instance variable and is returned as a result from the function. Further calls to the function will not create new instances, but will directly return the initial instance, stored in the instance variable. Discussion A little bit more advanced implementation of the Singleton pattern would be to have named object instances. For example, you might want to have different loggers. For example: Logger('database') returns an instance for logging database messages. Logger('http') returns an instance for logging HTTP requests. Further Reading You can learn more about the way the __new__ method works in the Python documentation .","title":"Convert a Python class to Singleton using decorator"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#convert-a-python-class-to-singleton-using-decorator","text":"In Python there are many ways to implement the Singleton Pattern . For example: Modules are singletons in Python. You could use java-like implementation, combined with factory method. There is another interesting pythonic way to turn any class into a singleton. You could use a Python decorator and apply it to the class you want to be a singleton. Probably more pythonic would be to use a meta class to create a singleton. This is another story. I will tell you this story soon.","title":"Convert a Python class to Singleton using decorator"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#python-singleton-decorator","text":"The decorator is changing the way new objects are created from the decorated class. Each time you are requesting a new object, you will get the same object again and again. Here is the definition of the singleton decorator: from functools import wraps def singleton(orig_cls): orig_new = orig_cls.__new__ instance = None @wraps(orig_cls.__new__) def __new__(cls, *args, **kwargs): nonlocal instance if instance is None: instance = orig_new(cls, *args, **kwargs) return instance orig_cls.__new__ = __new__ return orig_cls Here is an example usage: @singleton class Logger: def log(msg): print(msg) logger1 = Logger() logger2 = Logger() assert logger1 is logger2 In this example we have a simple Logger class with a single method log which logs a message. For simplicity our implementation is just printing the message to the standard output. We are creating two Logger objects - logger1 and logger2 we verify that the two variables are actually referring to the same object, using the assert statement.","title":"Python Singleton Decorator"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#try-it-out","text":"Here is a Jupyter notebook I created as GitHub gist . You can use this notebook to try the above example.","title":"Try it out"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#how-the-singleton-decorator-works","text":"This solution is based on the way Python creates new instances of a class. When a new instance is to be created, the special method of the class, called __new__ , is called to create the instance. The method should return the newly created instance. In this solution the decorator is overwriting the original __new__ method of the class so that it will return same instance each time it is called. When you add a decorator to a class, the decorator function is called once, receiving as a first argument the decorated class. The decorator stores a reference to the original __new__ method of the decorated class into a variable named orig_new , the original __new__ method is replaced with different implementation. The new implementation of the __new__ method is checking if an instance of the class has already been created. If this is the first call to the function the instance variable is not set. The original method referenced by the orig_new variable is called to create the initial instance of the class. The object is stored in in the instance variable and is returned as a result from the function. Further calls to the function will not create new instances, but will directly return the initial instance, stored in the instance variable.","title":"How the singleton decorator works?"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#discussion","text":"A little bit more advanced implementation of the Singleton pattern would be to have named object instances. For example, you might want to have different loggers. For example: Logger('database') returns an instance for logging database messages. Logger('http') returns an instance for logging HTTP requests.","title":"Discussion"},{"location":"python/design-patterns/python-singleton-pattern-decorator/#further-reading","text":"You can learn more about the way the __new__ method works in the Python documentation .","title":"Further Reading"},{"location":"python/kata/","text":"Python Kata Kata #1: Pipe and Filter in Python Kata #2: The Galaxy Empire salaries Kata #3: World population Kata #4: Hello Mars!","title":"Kata"},{"location":"python/kata/#python-kata","text":"Kata #1: Pipe and Filter in Python Kata #2: The Galaxy Empire salaries Kata #3: World population Kata #4: Hello Mars!","title":"Python Kata"},{"location":"python/kata/python-kata-galaxy-empire-salaries/","text":"Python Kata #2: The Galaxy Empire Salaries You are given the information about the salaries of all the employees in the Galaxy Empire. The information is a dataset in CSV format with header row. How would you compute the average, the count, and the minimum and the maximum values for the salary column?","title":"Python Kata #2: The Galaxy Empire Salaries"},{"location":"python/kata/python-kata-galaxy-empire-salaries/#python-kata-2-the-galaxy-empire-salaries","text":"You are given the information about the salaries of all the employees in the Galaxy Empire. The information is a dataset in CSV format with header row. How would you compute the average, the count, and the minimum and the maximum values for the salary column?","title":"Python Kata #2: The Galaxy Empire Salaries"},{"location":"python/kata/python-kata-hello-mars/","text":"Python Kata #4: Hello Mars! Me and my friends like to travel to other planets. To keep track on which planets we visited I created a Python VisitTracker class. class VisitTracker: visited = [] def __init__(self, name): self.name = name def visit(self, place): if place not in self.visited: self.visited.append(place) def list(self): print(\"====== \" + self.name + \"'s visits ======\") print(self.visited) I put all the Python source code in an Jupyter Notebook (I made it available for you as GitHub gist ) and started using it. Added recent planetary visits John and Jane made: John visited Saturn together with Jane John visited Mars Jane visited Jupiter Here is how I tracked this: johns_visits = VisitTracker('John') janes_visits = VisitTracker('Jane') #1 John visited Saturn together with Jane johns_visits.visit('Saturn') janes_visits.visit('Saturn') #2 John visited Mars johns_visits.visit('Mars') johns_visits.list() #3 Jane visited Jupiter janes_visits.visit('Jupiter') janes_visits.list() And the output was not quite what I was expecting: ====== John's visits ====== ['Saturn', 'Mars'] ====== Jane's visits ====== ['Saturn', 'Mars', 'Jupiter'] I was expecting: ====== John's visits ====== ['Saturn', 'Mars'] ====== Jane's visits ====== ['Saturn', 'Jupiter'] If you do not believe me - check the Jupyter Notebook with the Python code and the results in Colab . What went wrong?","title":"Python Kata #4: Hello Mars!"},{"location":"python/kata/python-kata-hello-mars/#python-kata-4-hello-mars","text":"Me and my friends like to travel to other planets. To keep track on which planets we visited I created a Python VisitTracker class. class VisitTracker: visited = [] def __init__(self, name): self.name = name def visit(self, place): if place not in self.visited: self.visited.append(place) def list(self): print(\"====== \" + self.name + \"'s visits ======\") print(self.visited) I put all the Python source code in an Jupyter Notebook (I made it available for you as GitHub gist ) and started using it. Added recent planetary visits John and Jane made: John visited Saturn together with Jane John visited Mars Jane visited Jupiter Here is how I tracked this: johns_visits = VisitTracker('John') janes_visits = VisitTracker('Jane') #1 John visited Saturn together with Jane johns_visits.visit('Saturn') janes_visits.visit('Saturn') #2 John visited Mars johns_visits.visit('Mars') johns_visits.list() #3 Jane visited Jupiter janes_visits.visit('Jupiter') janes_visits.list() And the output was not quite what I was expecting: ====== John's visits ====== ['Saturn', 'Mars'] ====== Jane's visits ====== ['Saturn', 'Mars', 'Jupiter'] I was expecting: ====== John's visits ====== ['Saturn', 'Mars'] ====== Jane's visits ====== ['Saturn', 'Jupiter'] If you do not believe me - check the Jupyter Notebook with the Python code and the results in Colab . What went wrong?","title":"Python Kata #4: Hello Mars!"},{"location":"python/kata/python-kata-pipe-and-filter/","text":"Python Kata #1: Pipe and Filter The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. filter - takes as an argument a bool function. Function is applied for each element in the input. If the filter function returns true, the element is placed in the output. Otherwise the element is dropped. To implement these patterns you might find useful the Python functions map , filter , `reduce (See Map, Filter and Reduce at Intermediate Python) Do not forget to test your solution! Here is an example what it might look like using the implementation. Example: Input: a list of strings, e.g. ['Lorem ipsum', 'dolorem costum'] Transformation 1: convert all items to lower case, e.g. ['lorem ipsum', 'dolorem costum'] Transformation 2: split each element into, e.g. ['lorem', 'ipsum', 'dolorem', 'costum'] Transformation 3: remove all words ending with 'em', e.g. ['ipsum', 'costum'] Python code: pipe = (Pipe(['Lorem ipsum', 'dolorem costum']) .map(lambda x: x.lower()) .flat_map(lambda x: x.split(' ')) .filter(lambda x: not x.endswith('em'))) for item in pipe: print(item) Output: ipsum costum","title":"Python Kata #1: Pipe and Filter"},{"location":"python/kata/python-kata-pipe-and-filter/#python-kata-1-pipe-and-filter","text":"The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. filter - takes as an argument a bool function. Function is applied for each element in the input. If the filter function returns true, the element is placed in the output. Otherwise the element is dropped. To implement these patterns you might find useful the Python functions map , filter , `reduce (See Map, Filter and Reduce at Intermediate Python) Do not forget to test your solution! Here is an example what it might look like using the implementation. Example: Input: a list of strings, e.g. ['Lorem ipsum', 'dolorem costum'] Transformation 1: convert all items to lower case, e.g. ['lorem ipsum', 'dolorem costum'] Transformation 2: split each element into, e.g. ['lorem', 'ipsum', 'dolorem', 'costum'] Transformation 3: remove all words ending with 'em', e.g. ['ipsum', 'costum'] Python code: pipe = (Pipe(['Lorem ipsum', 'dolorem costum']) .map(lambda x: x.lower()) .flat_map(lambda x: x.split(' ')) .filter(lambda x: not x.endswith('em'))) for item in pipe: print(item) Output: ipsum costum","title":"Python Kata #1: Pipe and Filter"},{"location":"python/kata/python-kata-world-population/","text":"Python Kata #3: World population You are involved in a campaign against a lethal virus. You just received the health status information for the whole world population in a CSV file with a header line. You have a to work on number of reports, generated from the dataset. Can you create a Python generator to help you with the reports? Each item returned from the generator is a namedtuple with fields corresponding to the columns of the CSV file.","title":"Python Kata #3: World population"},{"location":"python/kata/python-kata-world-population/#python-kata-3-world-population","text":"You are involved in a campaign against a lethal virus. You just received the health status information for the whole world population in a CSV file with a header line. You have a to work on number of reports, generated from the dataset. Can you create a Python generator to help you with the reports? Each item returned from the generator is a namedtuple with fields corresponding to the columns of the CSV file.","title":"Python Kata #3: World population"},{"location":"python/kata/drafts/python-kata-count-lines-in-multiple-files/","text":"Here is one possible solution .","title":"Python kata count lines in multiple files"},{"location":"python/kata/drafts/python-kata-how-would-you-test-this/","text":"Python Kata #?: How would you test this? You have to implement a function which reads a csv file and returns Python generator of rows. The first line of the csv file is a header. Each row is a namedtuple object with named fields according to the header. File must be closed after the generator is exhausted or stopped. Throws FormatError exception in case of parsing errors. from collections import namedtuple def read_csv(file_name): with open(file_name) as file: lines = (line for line in file) data_rows = (row for row in csv.reader(lines)) header_row = next(data_rows) Row = namedtuple('Row', header_row) rows = (Row(*row) for row in data_rows) return rows","title":"Python Kata #?: How would you test this?"},{"location":"python/kata/drafts/python-kata-how-would-you-test-this/#python-kata-how-would-you-test-this","text":"You have to implement a function which reads a csv file and returns Python generator of rows. The first line of the csv file is a header. Each row is a namedtuple object with named fields according to the header. File must be closed after the generator is exhausted or stopped. Throws FormatError exception in case of parsing errors. from collections import namedtuple def read_csv(file_name): with open(file_name) as file: lines = (line for line in file) data_rows = (row for row in csv.reader(lines)) header_row = next(data_rows) Row = namedtuple('Row', header_row) rows = (Row(*row) for row in data_rows) return rows","title":"Python Kata #?: How would you test this?"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/","text":"Iterate over hierarchical sources in Python Problem Scenario We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = ['a.txt', 'b.txt'] for file_name in file_list: with open(file_name, 'r') as file: for line in file.readline(): for word in line.split(' '): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() Wow. Not very easy to read already, but it serves the purpose. How about testing? You think - it is pretty straightforward. \"I did couple of exploration tests during development. I am pretty confident it works!\" Well ... wait to see what comes next. Day 1. New requirements: Stop words should be inserted into a table stop_words . Rest of the words are still going into the words table. Day 2. New requirements: Our database administrator has some concerns about the performance and wants that the commit statement happens after 1000 words has been inserted into each table. This means you keep track on number of inserted words in each table separately. When any of the table reaches 1000 you commit inserts for that table and reset the counter for that table. Day 3. New requirement: We should be able to extract words from Word files. Bug: words are not split on punctuation. Bug: multiple spaces cause empty words to be inserted. New requirement: words need to be stored in lower case. Week 5. You worked very hard, but you are still unable to finish the Word files processing. Your manager asks you to deliver without this feature. She also assigns three new developers to your project as new requirements arrive. You need to be able to process PDF files Week 25. Everything works more or less fine. Now the privacy team comes into the picture. Only approved files should be processed. You receive a list of approved files in a CSV file. You need to be able to OCR images You need to create a build pipeline for continuous integration The solution needs to implement unit tests with at least 70% coverage. Unit tests should be executed at build time. Year 1. Privacy team has provided REST API to validate the file eligibility for processing. Can you remember how the whole this hell thing works? Real-time processing! List of files is stored in a database table. It is updated constantly. Our solution needs to process each new file entry as soon as possible. We want that words are inserted not only into the database, but also sent to a message queue. What if? What if our processing looks like this: for word in extract_words(): process_word(word) Even better: db_processor = word_process.get_database_processor() extractor = word_extract.get() reader.when_new_word(processor.process) Solution Let's start with the original requirements. Can we make the code more readable","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#iterate-over-hierarchical-sources-in-python","text":"","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#problem","text":"","title":"Problem"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#scenario","text":"We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = ['a.txt', 'b.txt'] for file_name in file_list: with open(file_name, 'r') as file: for line in file.readline(): for word in line.split(' '): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() Wow. Not very easy to read already, but it serves the purpose. How about testing? You think - it is pretty straightforward. \"I did couple of exploration tests during development. I am pretty confident it works!\" Well ... wait to see what comes next. Day 1. New requirements: Stop words should be inserted into a table stop_words . Rest of the words are still going into the words table. Day 2. New requirements: Our database administrator has some concerns about the performance and wants that the commit statement happens after 1000 words has been inserted into each table. This means you keep track on number of inserted words in each table separately. When any of the table reaches 1000 you commit inserts for that table and reset the counter for that table. Day 3. New requirement: We should be able to extract words from Word files. Bug: words are not split on punctuation. Bug: multiple spaces cause empty words to be inserted. New requirement: words need to be stored in lower case. Week 5. You worked very hard, but you are still unable to finish the Word files processing. Your manager asks you to deliver without this feature. She also assigns three new developers to your project as new requirements arrive. You need to be able to process PDF files Week 25. Everything works more or less fine. Now the privacy team comes into the picture. Only approved files should be processed. You receive a list of approved files in a CSV file. You need to be able to OCR images You need to create a build pipeline for continuous integration The solution needs to implement unit tests with at least 70% coverage. Unit tests should be executed at build time. Year 1. Privacy team has provided REST API to validate the file eligibility for processing. Can you remember how the whole this hell thing works? Real-time processing! List of files is stored in a database table. It is updated constantly. Our solution needs to process each new file entry as soon as possible. We want that words are inserted not only into the database, but also sent to a message queue.","title":"Scenario"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#what-if","text":"What if our processing looks like this: for word in extract_words(): process_word(word) Even better: db_processor = word_process.get_database_processor() extractor = word_extract.get() reader.when_new_word(processor.process)","title":"What if?"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#solution","text":"Let's start with the original requirements. Can we make the code more readable","title":"Solution"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/","text":"Iterate over hierarchical sources in Python Problem We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = ['a.txt', 'b.txt'] for file_name in file_list: with open(file_name, 'r') as file: for line in file: for word in line.split(' '): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() How do you test such code? Maybe you use something I call 'evolutionary testing' - you start with the outer loop. Add a print statement, run a few times, comment the print statement. And you continue until it is done. Ok. This might work at the beginning. But how you test your code continuously? There are situations where you need to verify the code is working correctly. For example: New requirements - e.g. process PDF, image files etc. Dependency upgrade - e.g. changing to a higher version of Python Change the target database Issue fixing Error handling This code is already difficult to read and understand. You want to improve your code - make it testable, more readable and maintainable, more flexible and extensible. Discussion Look at the code. Isn't it doing too much things? Solution Let's start with the responsibilities. Iterate over a list of files Process a file - iterate over the lines from a file Get the lines from a file Process a line - iterate over the words in a line Get the words from a line Process a word - save a word into the database So we might come with an object-oriented solution like this: class WordLoader: def _get_line_words(self, line): return line.split(' ') def _get_file_lines(file_name): with open(file_name, 'r') as file: return f def _process_word(self, word): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() def _process_line(self, line): for word in self._get_line_words(line): self._process_word(word) def _process_file(self, file_name): for line in self._get_file_lines(file_name): self._process_line(line) def __call__(self, file_list): for file_name in file_list: self._process_file(file_name) Looks much better now. We can test each method in isolation with unit tests. However I still have some concerns. The WordLoader class we created takes too much responsibilities. What is going on is still not very visible. To understand what is going on, I need to dig into the whole chain of execution. Can I make my code more expressive so that I can read the code from the very beginning like this: \"Insert all the words from a list of files into the database.\" Maybe my main code could look like this? for word in words_from_files: save_word(word) Where the words_from_files comes from? files = get_files_from_name_list(file_list) lines_from_files = get_lines_from_files(files) words_from_files = get_words_from_lines(lines_from_files) Wait! You are going to load all these terabytes and petabytes into the memory? Good point. Not necessarily. Instead of returning list, I could use generators. def get_files_from_name_list(filename_list): for filename in filename_list: with open(filename, 'r') as file: yield file def get_lines_from_files(files): for file in files: for line in file: yield file def get_words_from_lines(lines): for line in lines: for word in line.split(' ') yield word The code is much more expressive now. There are two things I do not like: functions look almost the same functions are doing more than one thing. Let's take get_lines_from_files as an example. It iterates over the files in a list, unpacks the file into lines, using split() method and iterates over the resulting words. def unpack_containers(containers, unpack): for container in containers: for item in unpack(container): yield item def unpack_filename(filename): with open(filename, 'r') as f: yield [f] def unpack_file_lines(file): return file def unpack_line_words(line): for word in line.split(' '): yeild word files = unpack_containers(file_list, lambda filename: [open(filename, 'r')]) lines_from_files = get_lines_from_files(files) words_from_files = get_words_from_lines(lines_from_files) files = map(open_file, file_list) lines = chain.from_iterable(files) words = chain.from_iterable(map(lambda line: line.split(' '), lines)) def open_file(filename): with open(filename, 'r') as f: yield f def get_words_from_line(line): return line.split(' ') def flat_map(function, iterable, *arg): arg.insert(0, iterable) chain.from_iterable(map(function, *arg)) lines = map(open_file, file_list) words = flat_map(get_words_from_line, lines) def list_new_events(): with db.cursor() as c: c.execute(\"SELECT * FROM events WHERE status='N'\") yield c.fetchall() def repeat(function, before=None, after=None): while True: if before is not None: before() yield function() if after is not None: after() def reset_process(): pass def process(event): pass event_stream = chain.from_iterable(repeat, after=reset_process) for event in event_stream: process(event)","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/#iterate-over-hierarchical-sources-in-python","text":"","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/#problem","text":"We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = ['a.txt', 'b.txt'] for file_name in file_list: with open(file_name, 'r') as file: for line in file: for word in line.split(' '): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() How do you test such code? Maybe you use something I call 'evolutionary testing' - you start with the outer loop. Add a print statement, run a few times, comment the print statement. And you continue until it is done. Ok. This might work at the beginning. But how you test your code continuously? There are situations where you need to verify the code is working correctly. For example: New requirements - e.g. process PDF, image files etc. Dependency upgrade - e.g. changing to a higher version of Python Change the target database Issue fixing Error handling This code is already difficult to read and understand. You want to improve your code - make it testable, more readable and maintainable, more flexible and extensible.","title":"Problem"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/#discussion","text":"Look at the code. Isn't it doing too much things?","title":"Discussion"},{"location":"python/kata/drafts/python-kata-iterate-over-hierarchical-sources/#solution","text":"Let's start with the responsibilities. Iterate over a list of files Process a file - iterate over the lines from a file Get the lines from a file Process a line - iterate over the words in a line Get the words from a line Process a word - save a word into the database So we might come with an object-oriented solution like this: class WordLoader: def _get_line_words(self, line): return line.split(' ') def _get_file_lines(file_name): with open(file_name, 'r') as file: return f def _process_word(self, word): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() def _process_line(self, line): for word in self._get_line_words(line): self._process_word(word) def _process_file(self, file_name): for line in self._get_file_lines(file_name): self._process_line(line) def __call__(self, file_list): for file_name in file_list: self._process_file(file_name) Looks much better now. We can test each method in isolation with unit tests. However I still have some concerns. The WordLoader class we created takes too much responsibilities. What is going on is still not very visible. To understand what is going on, I need to dig into the whole chain of execution. Can I make my code more expressive so that I can read the code from the very beginning like this: \"Insert all the words from a list of files into the database.\" Maybe my main code could look like this? for word in words_from_files: save_word(word) Where the words_from_files comes from? files = get_files_from_name_list(file_list) lines_from_files = get_lines_from_files(files) words_from_files = get_words_from_lines(lines_from_files) Wait! You are going to load all these terabytes and petabytes into the memory? Good point. Not necessarily. Instead of returning list, I could use generators. def get_files_from_name_list(filename_list): for filename in filename_list: with open(filename, 'r') as file: yield file def get_lines_from_files(files): for file in files: for line in file: yield file def get_words_from_lines(lines): for line in lines: for word in line.split(' ') yield word The code is much more expressive now. There are two things I do not like: functions look almost the same functions are doing more than one thing. Let's take get_lines_from_files as an example. It iterates over the files in a list, unpacks the file into lines, using split() method and iterates over the resulting words. def unpack_containers(containers, unpack): for container in containers: for item in unpack(container): yield item def unpack_filename(filename): with open(filename, 'r') as f: yield [f] def unpack_file_lines(file): return file def unpack_line_words(line): for word in line.split(' '): yeild word files = unpack_containers(file_list, lambda filename: [open(filename, 'r')]) lines_from_files = get_lines_from_files(files) words_from_files = get_words_from_lines(lines_from_files) files = map(open_file, file_list) lines = chain.from_iterable(files) words = chain.from_iterable(map(lambda line: line.split(' '), lines)) def open_file(filename): with open(filename, 'r') as f: yield f def get_words_from_line(line): return line.split(' ') def flat_map(function, iterable, *arg): arg.insert(0, iterable) chain.from_iterable(map(function, *arg)) lines = map(open_file, file_list) words = flat_map(get_words_from_line, lines) def list_new_events(): with db.cursor() as c: c.execute(\"SELECT * FROM events WHERE status='N'\") yield c.fetchall() def repeat(function, before=None, after=None): while True: if before is not None: before() yield function() if after is not None: after() def reset_process(): pass def process(event): pass event_stream = chain.from_iterable(repeat, after=reset_process) for event in event_stream: process(event)","title":"Solution"},{"location":"python/kata/drafts/python-kata-pipe-and-filter-solution/","text":"Pipe and Filter in Python The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. Here is an example what it might look like using the implementation. Example: Input: a list of integers [4,6,9] Transformation 1: divide each integer by 2 and convert to integer Transformation 2: convert each item into a sequence, e.g. 1 becomes [1], 2 becomes [2, 2], 3 becomes [3,3,3], etc. Flatten the result. Python code: pipe = (Pipe([4,6,9]) .map(lambda x: x/2) .flat_map(lambda x: str(x)*int(x))) for item in pipe: print(item) Output: 2 2 3 3 3 4 4 4 4","title":"Pipe and Filter in Python"},{"location":"python/kata/drafts/python-kata-pipe-and-filter-solution/#pipe-and-filter-in-python","text":"The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. Here is an example what it might look like using the implementation. Example: Input: a list of integers [4,6,9] Transformation 1: divide each integer by 2 and convert to integer Transformation 2: convert each item into a sequence, e.g. 1 becomes [1], 2 becomes [2, 2], 3 becomes [3,3,3], etc. Flatten the result. Python code: pipe = (Pipe([4,6,9]) .map(lambda x: x/2) .flat_map(lambda x: str(x)*int(x))) for item in pipe: print(item) Output: 2 2 3 3 3 4 4 4 4","title":"Pipe and Filter in Python"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/","text":"Public Transportation - Python Kata Solution Solution 1 Your solution might look like the following: import os STAGING_DIR = '.' while True: for filename in os.listdir(STAGING_DIR): filepath = os.path.join(STAGING_DIR, filename) if os.path.isfile(filepath): with open(filepath, 'r') as file: for line in file: for word in line.split(' '): save_word(word) Although this is not the worst solution I have seen, it has some weaknesses: Deep statement nesting at 6 nesting levels. This makes the code very difficult. Where should I add the sleep calls? No error handling. If you add try-except blocks, nesting will become even deeper. Difficult to test. You might have followed the \"growing onion layers\" approach which helped in the initial development, but how would you test changes, bug fixes, Python version upgrades? Difficult to reuse. Actually templating or as I prefer calling it - copy/paste/edit - is the only way to reuse such a code. Solution 2 class WordLoader: def _get_line_words(self, line): return line.split(' ') def _get_file_lines(file_name): with open(file_name, 'r') as file: yield file def _process_word(self, word): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() def _process_line(self, line): for word in self._get_line_words(line): self._process_word(word) def _process_file(self, file_name): for line in self._get_file_lines(file_name): self._process_line(line) def __call__(self, file_list): for file_name in file_list: self._process_file(file_name) load_file_list = FileLoader() while True: file_list = [f for f in os.listdir(STAGING_DIR) if os.path.isfile(os.path.join(STAGING_DIR, f))] load_file_list(file_list) Solution 3 def repeat(function, before=None, after=None): while True: try: value = function() except StopIteration: break if before is not None: before(value) yield value if after is not None: after(value) def pipeline(iterable, function, before=None, after=None): for item in terable: if before is not None: item = before(item) function(item) if after is not None: after(item) def ls_staged_files(): file_list = [f for f in os.listdir(STAGING_DIR) if \\ os.path.isfile(os.path.join(STAGING_DIR, f))] return file_list def after_file_list(item): time.sleep(3) def get_lines_from_files(files): for filename in files: filepath = os.path.join(STAGING_DIR, filename) with open(filepath, 'r') as file: for line in file: yield line def get_words_from_lines(lines): for line in lines: for word in line.split(' '): yield word def open_files(filenames): for filename in filenames: filepath = os.path.join(STAGING_DIR, filename) with open(filepath, 'r') as file: yield file STAGING_DIR = '.' filelist_sequence = repeat(ls_staged_files, after=after_file_list) filename_sequence = itertools.chain.from_iterable(filelist_sequence) file_sequence = open_files(filename_sequence) file_sequence = itertools.chain.from_iterable(repeat(ls_staged_files, after=after_file_list)) line_sequence = get_lines_from_files(file_sequence) word_sequence = get_words_from_lines(line_sequence) for word in word_sequence: print(word)","title":"Public Transportation - Python Kata Solution"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/#public-transportation-python-kata-solution","text":"","title":"Public Transportation - Python Kata Solution"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/#solution-1","text":"Your solution might look like the following: import os STAGING_DIR = '.' while True: for filename in os.listdir(STAGING_DIR): filepath = os.path.join(STAGING_DIR, filename) if os.path.isfile(filepath): with open(filepath, 'r') as file: for line in file: for word in line.split(' '): save_word(word) Although this is not the worst solution I have seen, it has some weaknesses: Deep statement nesting at 6 nesting levels. This makes the code very difficult. Where should I add the sleep calls? No error handling. If you add try-except blocks, nesting will become even deeper. Difficult to test. You might have followed the \"growing onion layers\" approach which helped in the initial development, but how would you test changes, bug fixes, Python version upgrades? Difficult to reuse. Actually templating or as I prefer calling it - copy/paste/edit - is the only way to reuse such a code.","title":"Solution 1"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/#solution-2","text":"class WordLoader: def _get_line_words(self, line): return line.split(' ') def _get_file_lines(file_name): with open(file_name, 'r') as file: yield file def _process_word(self, word): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() def _process_line(self, line): for word in self._get_line_words(line): self._process_word(word) def _process_file(self, file_name): for line in self._get_file_lines(file_name): self._process_line(line) def __call__(self, file_list): for file_name in file_list: self._process_file(file_name) load_file_list = FileLoader() while True: file_list = [f for f in os.listdir(STAGING_DIR) if os.path.isfile(os.path.join(STAGING_DIR, f))] load_file_list(file_list)","title":"Solution 2"},{"location":"python/kata/drafts/python-kata-public-transportation-solution/#solution-3","text":"def repeat(function, before=None, after=None): while True: try: value = function() except StopIteration: break if before is not None: before(value) yield value if after is not None: after(value) def pipeline(iterable, function, before=None, after=None): for item in terable: if before is not None: item = before(item) function(item) if after is not None: after(item) def ls_staged_files(): file_list = [f for f in os.listdir(STAGING_DIR) if \\ os.path.isfile(os.path.join(STAGING_DIR, f))] return file_list def after_file_list(item): time.sleep(3) def get_lines_from_files(files): for filename in files: filepath = os.path.join(STAGING_DIR, filename) with open(filepath, 'r') as file: for line in file: yield line def get_words_from_lines(lines): for line in lines: for word in line.split(' '): yield word def open_files(filenames): for filename in filenames: filepath = os.path.join(STAGING_DIR, filename) with open(filepath, 'r') as file: yield file STAGING_DIR = '.' filelist_sequence = repeat(ls_staged_files, after=after_file_list) filename_sequence = itertools.chain.from_iterable(filelist_sequence) file_sequence = open_files(filename_sequence) file_sequence = itertools.chain.from_iterable(repeat(ls_staged_files, after=after_file_list)) line_sequence = get_lines_from_files(file_sequence) word_sequence = get_words_from_lines(line_sequence) for word in word_sequence: print(word)","title":"Solution 3"},{"location":"python/kata/drafts/python-kata-public-transportation/","text":"Public Transportation - Python Kata Imagine you live in Amsterdam and you want to travel to Paris. Of course you want to travel as soon as possible, but you have to take in account the railways timetable. Railways work in batch mode. They come on a schedule, pick everybody from the train station and move them. In our software development practice we face similar problems every day. Here is a similar one. Problem Files are being delivered by multiple source systems into a staging folder. These files are our passengers. You need to create a solution which periodically is taking a list of available files and is processing them by extracting the words from the files and inserting into a database table. This is processing solution is our train. In fact what we are creating is a pipeline. The pipeline is using batches - the list of available files. Think about following: How do you test your solution? How do you handle exceptions? How easy is for other people (or you after a year) to read and understand your solution? How would you add pre- and post-processing of words to your pipeline? For example, as pre-processing you might need to convert the word to lower case, strip punctuation characters. Post-processing might be to sleep for 2 seconds. How would you add pre- and post-processing of batches to your pipeline? For example, as post-processing, you might want to sleep for 20 seconds. How would you reuse this solution in other scenarios? For example - your staging is a database table and as processing an email is sent.","title":"Public Transportation - Python Kata"},{"location":"python/kata/drafts/python-kata-public-transportation/#public-transportation-python-kata","text":"Imagine you live in Amsterdam and you want to travel to Paris. Of course you want to travel as soon as possible, but you have to take in account the railways timetable. Railways work in batch mode. They come on a schedule, pick everybody from the train station and move them. In our software development practice we face similar problems every day. Here is a similar one.","title":"Public Transportation - Python Kata"},{"location":"python/kata/drafts/python-kata-public-transportation/#problem","text":"Files are being delivered by multiple source systems into a staging folder. These files are our passengers. You need to create a solution which periodically is taking a list of available files and is processing them by extracting the words from the files and inserting into a database table. This is processing solution is our train. In fact what we are creating is a pipeline. The pipeline is using batches - the list of available files. Think about following: How do you test your solution? How do you handle exceptions? How easy is for other people (or you after a year) to read and understand your solution? How would you add pre- and post-processing of words to your pipeline? For example, as pre-processing you might need to convert the word to lower case, strip punctuation characters. Post-processing might be to sleep for 2 seconds. How would you add pre- and post-processing of batches to your pipeline? For example, as post-processing, you might want to sleep for 20 seconds. How would you reuse this solution in other scenarios? For example - your staging is a database table and as processing an email is sent.","title":"Problem"},{"location":"python/misc/","text":"Misc Python Topics Document REST APIs based on Python Django's Rest Framework on October 12 th , 2020, in Python :: Misc Topics Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project on October 12 th , 2020, in Python :: Misc Topics Decode and Validate Azure Active Directory Token in Python on August 31 st , 2020, in Python :: Misc Topics Truncate table with pyodbc on August 17 th , 2020, in Python :: Misc Topics","title":"Misc Python Topics"},{"location":"python/misc/#misc-python-topics","text":"Document REST APIs based on Python Django's Rest Framework on October 12 th , 2020, in Python :: Misc Topics Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project on October 12 th , 2020, in Python :: Misc Topics Decode and Validate Azure Active Directory Token in Python on August 31 st , 2020, in Python :: Misc Topics Truncate table with pyodbc on August 17 th , 2020, in Python :: Misc Topics","title":"Misc Python Topics"},{"location":"python/misc/python-azure-ad-token-decode-validate/","text":"Decode and Validate JWT Token from Azure Active Directory in Python Problem You need to decode and validate JWT Token issued by Azure Active Directory. Example scenario is where you have a Web Application which is using BFF (Backend for Frontend) API. Users are authenticated by the front-end application using Azure AD and the token is forwarded to the BFF API. The BFF API needs to validate the received token since the client is outside of its trust boundary. Solution Use pyjwt and cryptography packages. I have created a small package aadtoken to help with getting the Azure Active Directory public key and decode the token using, using pyjwt and cryptography . Further to decode the token use the jwt.decode function from the pyjwat package. All the sources are available in GitHub . Here I am providing only an example how to use the aadtoken helper package along with jwt.decode : import os import sys import jwt from aadtoken import get_public_key client_id = os.environ.get('CLIENT_ID', '<your-webapp-id-goes-here>') tenant_id = os.environ.get('TENANT_ID', '<your-tenant-id-goes-here>') if len(sys.argv) > 1: token = sys.argv[1] else: token = os.environ.get('TOKEN', \"<your-token-goes-here>\") issuer = 'https://sts.windows.net/{tenant_id}/'.format(tenant_id=tenant_id) public_key = get_public_key(token) decoded = jwt.decode(token, public_key, verify=True, algorithms=['RS256'], audience=[client_id], issuer=issuer) print(decoded) You need to replace the placeholders with actual values. Alternatively you could use environment variables to define the client id, the tenant id and the token. The token id can also be passed as a command line argument: python demo.py <your-token-goes-here> Discussion This solution is based on the Validating JSON web tokens (JWTs) from Azure AD, in Python publication by Roberto Prevato. The solution defines a package which is responsible for discovering the Azure AD endpoints and getting the Azure Active Directory's public key. Requests to Azure Active Directory discovery and keys endpoints are cached. The most important function exported by the package is get_public_key(<token>, [<tenant_id>]) . For given token and tenant ID the function returns the Azure Active Directory public key. The key is used by the jwt.decode function from the pyjwat package to validate and decode the token.","title":"Python azure ad token decode validate"},{"location":"python/misc/python-azure-ad-token-decode-validate/#decode-and-validate-jwt-token-from-azure-active-directory-in-python","text":"","title":"Decode and Validate JWT Token from Azure Active Directory in Python"},{"location":"python/misc/python-azure-ad-token-decode-validate/#problem","text":"You need to decode and validate JWT Token issued by Azure Active Directory. Example scenario is where you have a Web Application which is using BFF (Backend for Frontend) API. Users are authenticated by the front-end application using Azure AD and the token is forwarded to the BFF API. The BFF API needs to validate the received token since the client is outside of its trust boundary.","title":"Problem"},{"location":"python/misc/python-azure-ad-token-decode-validate/#solution","text":"Use pyjwt and cryptography packages. I have created a small package aadtoken to help with getting the Azure Active Directory public key and decode the token using, using pyjwt and cryptography . Further to decode the token use the jwt.decode function from the pyjwat package. All the sources are available in GitHub . Here I am providing only an example how to use the aadtoken helper package along with jwt.decode : import os import sys import jwt from aadtoken import get_public_key client_id = os.environ.get('CLIENT_ID', '<your-webapp-id-goes-here>') tenant_id = os.environ.get('TENANT_ID', '<your-tenant-id-goes-here>') if len(sys.argv) > 1: token = sys.argv[1] else: token = os.environ.get('TOKEN', \"<your-token-goes-here>\") issuer = 'https://sts.windows.net/{tenant_id}/'.format(tenant_id=tenant_id) public_key = get_public_key(token) decoded = jwt.decode(token, public_key, verify=True, algorithms=['RS256'], audience=[client_id], issuer=issuer) print(decoded) You need to replace the placeholders with actual values. Alternatively you could use environment variables to define the client id, the tenant id and the token. The token id can also be passed as a command line argument: python demo.py <your-token-goes-here>","title":"Solution"},{"location":"python/misc/python-azure-ad-token-decode-validate/#discussion","text":"This solution is based on the Validating JSON web tokens (JWTs) from Azure AD, in Python publication by Roberto Prevato. The solution defines a package which is responsible for discovering the Azure AD endpoints and getting the Azure Active Directory's public key. Requests to Azure Active Directory discovery and keys endpoints are cached. The most important function exported by the package is get_public_key(<token>, [<tenant_id>]) . For given token and tenant ID the function returns the Azure Active Directory public key. The key is used by the jwt.decode function from the pyjwat package to validate and decode the token.","title":"Discussion"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/","text":"Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project Here is step-by-step recipe on how to add auto generated documentation for your REST API in OpenAPI format and enable SwaggerUI. The recipe is based on Document your API tutorial from Django Rest Framework. Step 1: Create documentation application python manage.py startapp docs Step 2. Register documentation endpoints Create urls.py in the api_docs application: from django.urls import path from django.views.generic import TemplateView from rest_framework.schemas import get_schema_view from rest_framework import renderers urlpatterns = [ path('docs/', TemplateView.as_view( template_name='swagger-ui.html', extra_context={'schema_url':'openapi-schema-yaml'} ), name='swagger-ui'), path('openapi.yaml', get_schema_view( title=\"Best API Service\", renderer_classes=[renderers.OpenAPIRenderer] ), name='openapi-schema-yaml'), path('openapi.json', get_schema_view( title=\"Best API Service\", renderer_classes = [renderers.JSONOpenAPIRenderer], ), name='openapi-schema-json'), ] This will register three endpoints: /openapi.json - OpenAPI documentation in JSON format /openapi.yaml - OpenAPI documentation in YAML format /docs - Swagger UI, based on the openapi.yaml Step 3. Create the Swagger UI template Create a swagger-ui.html file in the api_docs application's templates directory: <!DOCTYPE html> <html> <head> <title>Swagger</title> <meta charset=\"utf-8\"/> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <link rel=\"stylesheet\" type=\"text/css\" href=\"//unpkg.com/swagger-ui-dist@3/swagger-ui.css\" /> </head> <body> <div id=\"swagger-ui\"></div> <script src=\"//unpkg.com/swagger-ui-dist@3/swagger-ui-bundle.js\"></script> <script> const ui = SwaggerUIBundle({ url: \"{% url schema_url %}\", dom_id: '#swagger-ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIBundle.SwaggerUIStandalonePreset ], layout: \"BaseLayout\", requestInterceptor: (request) => { request.headers['X-CSRFToken'] = \"{{ csrf_token }}\" return request; } }) </script> </body> </html> Step 4. Register the api_docs application with the Django project To register the api_docs application in the Django project you need to modify the project's settings.py : INSTALLED_APPS = [ # ... 'rest_framework', 'docs', # ... ] Step 5. Add documentation URLs to Django project urls Modify the urls.py file for your Django project. Make sure in contains include('api_docs.urls') : from django.urls import path, include urlpatterns = [ # ... path('', include('api_docs.urls')), # ... ]","title":"Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#add-opeanapi-documentation-and-swaggerui-to-your-django-rest-framework-project","text":"Here is step-by-step recipe on how to add auto generated documentation for your REST API in OpenAPI format and enable SwaggerUI. The recipe is based on Document your API tutorial from Django Rest Framework.","title":"Add OpeanAPI documentation and SwaggerUI to your Django Rest Framework project"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-1-create-documentation-application","text":"python manage.py startapp docs","title":"Step 1: Create documentation application"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-2-register-documentation-endpoints","text":"Create urls.py in the api_docs application: from django.urls import path from django.views.generic import TemplateView from rest_framework.schemas import get_schema_view from rest_framework import renderers urlpatterns = [ path('docs/', TemplateView.as_view( template_name='swagger-ui.html', extra_context={'schema_url':'openapi-schema-yaml'} ), name='swagger-ui'), path('openapi.yaml', get_schema_view( title=\"Best API Service\", renderer_classes=[renderers.OpenAPIRenderer] ), name='openapi-schema-yaml'), path('openapi.json', get_schema_view( title=\"Best API Service\", renderer_classes = [renderers.JSONOpenAPIRenderer], ), name='openapi-schema-json'), ] This will register three endpoints: /openapi.json - OpenAPI documentation in JSON format /openapi.yaml - OpenAPI documentation in YAML format /docs - Swagger UI, based on the openapi.yaml","title":"Step 2. Register documentation endpoints"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-3-create-the-swagger-ui-template","text":"Create a swagger-ui.html file in the api_docs application's templates directory: <!DOCTYPE html> <html> <head> <title>Swagger</title> <meta charset=\"utf-8\"/> <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"> <link rel=\"stylesheet\" type=\"text/css\" href=\"//unpkg.com/swagger-ui-dist@3/swagger-ui.css\" /> </head> <body> <div id=\"swagger-ui\"></div> <script src=\"//unpkg.com/swagger-ui-dist@3/swagger-ui-bundle.js\"></script> <script> const ui = SwaggerUIBundle({ url: \"{% url schema_url %}\", dom_id: '#swagger-ui', presets: [ SwaggerUIBundle.presets.apis, SwaggerUIBundle.SwaggerUIStandalonePreset ], layout: \"BaseLayout\", requestInterceptor: (request) => { request.headers['X-CSRFToken'] = \"{{ csrf_token }}\" return request; } }) </script> </body> </html>","title":"Step 3. Create the Swagger UI template"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-4-register-the-api_docs-application-with-the-django-project","text":"To register the api_docs application in the Django project you need to modify the project's settings.py : INSTALLED_APPS = [ # ... 'rest_framework', 'docs', # ... ]","title":"Step 4. Register the api_docs application with the Django project"},{"location":"python/misc/python-django-rest-framework-opeanapi-swagger-documentation/#step-5-add-documentation-urls-to-django-project-urls","text":"Modify the urls.py file for your Django project. Make sure in contains include('api_docs.urls') : from django.urls import path, include urlpatterns = [ # ... path('', include('api_docs.urls')), # ... ]","title":"Step 5. Add documentation URLs to Django project urls"},{"location":"python/misc/python-django-rest-framework-openapi-documentation/","text":"Document REST APIs based on Python Django's Rest Framework Function Views When using object-oriented implementation for your REST API, most of the documentation can be generated automatically from serializers and models you have created. When using function based views, you have almost no flexibility for creating OpenAPI documentation for your REST API. The typical signature for a view function is: from rest_framework.decorators import api_view @api_view(['GET', 'POST']) def hello_world(request): ''' get: get greeting post: post a greeting ''' return Response({\"message\": \"Hello, world!\"}) The @api_view decorator wraps your view function into an APIView subclass. You can define description for all the methods, supported by the function in the function's docstring. This is practically all the supported documentation. Fortunately Django Rest Framework provides an extension point through the @schema decorator. I have created a quick and easy solution for OpenAPI generation, based on the @schema decorator. Custom AutoSchema class The solution parses the view function docstring and uses it in the process of the automatic documentation generation. The following AutoDocstringSchema class parses the yaml from the view docstring and returns the operations and components defined. To build the documentation, the AutoSchema methods are invoked first and the result is combined with the documentation from the docstring. If the docstring fails to parse as yaml, it is ignored. from rest_framework.schemas.openapi import AutoSchema class AutoDocstringSchema(AutoSchema): @property def documentation(self): if not hasattr(self, '_documentation'): try: self._documentation = yaml.safe_load(self.view.__doc__) except yaml.scanner.ScannerError: self._documentation = {} return self._documentation def get_components(self, path, method): components = super().get_components(path, method) doc_components = self.documentation.get('components', {}) components.update(doc_components) return components def get_operation(self, path, method): operation = super().get_operation( path, method) doc_operation = self.documentation.get(method.lower(), {}) operation.update(doc_operation) return operation Here is an example of function views, documented using yaml docstring: from rest_framework.decorators import api_view, schema @api_view(['GET']) @schema(AutoDocstringSchema()) def list_todos(request): ''' get: description: List todo items, stored in the database. summary: List todo items ''' # ... @api_view(['GET']) @schema(AutoDocstringSchema()) def get_todo_item(request, id): ''' get: description: Retrieve a todo item definition summary: Retrieve todo item parameters: - name: id in: path description: ToDo item ID schema: type: string responses: '200': description: Todo item successfully retrieved content: 'application/json': {} ''' # ... Multiple request methods are supported. You can have a view function whish supports GET, POST, etc. requests at the same time. To document any of them, you define corresponding documentation, using lowercase method name. You can also add definitions for components. If your docstring has components section, the components defined in this section will be added to the API components. Reference Django Rest Framework - Schema Django Rest Framework - Document your API OpenAPI Specification v.3.0.2","title":"Document REST APIs based on Python Django's Rest Framework Function Views"},{"location":"python/misc/python-django-rest-framework-openapi-documentation/#document-rest-apis-based-on-python-djangos-rest-framework-function-views","text":"When using object-oriented implementation for your REST API, most of the documentation can be generated automatically from serializers and models you have created. When using function based views, you have almost no flexibility for creating OpenAPI documentation for your REST API. The typical signature for a view function is: from rest_framework.decorators import api_view @api_view(['GET', 'POST']) def hello_world(request): ''' get: get greeting post: post a greeting ''' return Response({\"message\": \"Hello, world!\"}) The @api_view decorator wraps your view function into an APIView subclass. You can define description for all the methods, supported by the function in the function's docstring. This is practically all the supported documentation. Fortunately Django Rest Framework provides an extension point through the @schema decorator. I have created a quick and easy solution for OpenAPI generation, based on the @schema decorator.","title":"Document REST APIs based on Python Django's Rest Framework Function Views"},{"location":"python/misc/python-django-rest-framework-openapi-documentation/#custom-autoschema-class","text":"The solution parses the view function docstring and uses it in the process of the automatic documentation generation. The following AutoDocstringSchema class parses the yaml from the view docstring and returns the operations and components defined. To build the documentation, the AutoSchema methods are invoked first and the result is combined with the documentation from the docstring. If the docstring fails to parse as yaml, it is ignored. from rest_framework.schemas.openapi import AutoSchema class AutoDocstringSchema(AutoSchema): @property def documentation(self): if not hasattr(self, '_documentation'): try: self._documentation = yaml.safe_load(self.view.__doc__) except yaml.scanner.ScannerError: self._documentation = {} return self._documentation def get_components(self, path, method): components = super().get_components(path, method) doc_components = self.documentation.get('components', {}) components.update(doc_components) return components def get_operation(self, path, method): operation = super().get_operation( path, method) doc_operation = self.documentation.get(method.lower(), {}) operation.update(doc_operation) return operation Here is an example of function views, documented using yaml docstring: from rest_framework.decorators import api_view, schema @api_view(['GET']) @schema(AutoDocstringSchema()) def list_todos(request): ''' get: description: List todo items, stored in the database. summary: List todo items ''' # ... @api_view(['GET']) @schema(AutoDocstringSchema()) def get_todo_item(request, id): ''' get: description: Retrieve a todo item definition summary: Retrieve todo item parameters: - name: id in: path description: ToDo item ID schema: type: string responses: '200': description: Todo item successfully retrieved content: 'application/json': {} ''' # ... Multiple request methods are supported. You can have a view function whish supports GET, POST, etc. requests at the same time. To document any of them, you define corresponding documentation, using lowercase method name. You can also add definitions for components. If your docstring has components section, the components defined in this section will be added to the API components.","title":"Custom AutoSchema class"},{"location":"python/misc/python-django-rest-framework-openapi-documentation/#reference","text":"Django Rest Framework - Schema Django Rest Framework - Document your API OpenAPI Specification v.3.0.2","title":"Reference"},{"location":"python/misc/python-pyodbc-truncate-table/","text":"Truncate table with pyodbc Problem You need to truncate a table using pyodbc . Solution Here is an example of a function to truncate a database table, using pyodbc connection. You can find the full source in GitHub . def truncate_table(table_ref, dbc): try: with dbc.cursor() as cursor: cursor.execute(f'TRUNCATE TABLE {table_ref}') cursor.commit() except Exception as err: dbc.rollback() raise err Testing the solution Although the function is simple, it needs testing. The function should perform two steps: Truncate the table, executing TRUNCATE TABLE sql statement Commit the transaction This is the happy flow. In addition to the happy flow there is an exception flow which happens when pyodbc fails to execute the TRUNCATE TABLE sql statement. Here is a sample implementation of the unit tests that cover above scenarios. import unittest from unittest import mock from pyodbc_functions import truncate_table class Test_function_truncate_table(unittest.TestCase): def fix_dbc(self): dbc = mock.MagicMock() return dbc def test_truncate_table_calls_proper_methods_given_database_execute_is_successful(self): dbc = self.fix_dbc() truncate_table('users', dbc) with dbc.cursor() as cursor: cursor.assert_has_calls([ mock.call.execute('TRUNCATE TABLE users'), mock.call.commit() ]) def test_truncate_table_calls_rollback_on_and_propagates_exception_given_database_execute_fails(self): dbc = self.fix_dbc() with dbc.cursor() as cursor: cursor.execute.side_effect = Exception('bad boy') with self.assertRaises(Exception) as excinfo: truncate_table('users', dbc) self.assertEqual('bad boy', str(excinfo.exception)) cursor.asset_has_calls([ mock.call.execute(mock.call.ANY), mock.call.rollback() ]) We use mock database connection. In the happy flow test we pass mock database connection to the truncate_table function. Once the function is executed, we assert that following steps were made in a sequence: execute was called on the database cursor with proper SQL statement as argument commit with no arguments was called on the database cursor. In the exception flow test, we again use a mock database connection, but this time we configure the execute method of the cursor to throw an exception. We make sure the exception is propagated with assertRaises() unittest assert. We also check that the message of the exception is preserved. We verify that the flow is calling: execute method of the cursor - we do not verify the arguments since we already validated this in the happy flow. rollback method on the database connection. Discussion You can call the execute method on a connection cursor directly, but it is always better to move the code into a separate routine: provides reusability - you have a tested piece of code that can be used everywhere. improves the readability of the code - you create your own idioms or dictionary which make your code more expressive. improves the testability of the code - imagine your code truncates the table in the middle of 200+ line code fragment. How would you test it works correctly? How would you cover both scenarios? isolates your code from the external system - one of the benefits of this isolation is that you can unit test your code. You can find the pyodbc documentation here .","title":"Truncate table with pyodbc"},{"location":"python/misc/python-pyodbc-truncate-table/#truncate-table-with-pyodbc","text":"","title":"Truncate table with pyodbc"},{"location":"python/misc/python-pyodbc-truncate-table/#problem","text":"You need to truncate a table using pyodbc .","title":"Problem"},{"location":"python/misc/python-pyodbc-truncate-table/#solution","text":"Here is an example of a function to truncate a database table, using pyodbc connection. You can find the full source in GitHub . def truncate_table(table_ref, dbc): try: with dbc.cursor() as cursor: cursor.execute(f'TRUNCATE TABLE {table_ref}') cursor.commit() except Exception as err: dbc.rollback() raise err","title":"Solution"},{"location":"python/misc/python-pyodbc-truncate-table/#testing-the-solution","text":"Although the function is simple, it needs testing. The function should perform two steps: Truncate the table, executing TRUNCATE TABLE sql statement Commit the transaction This is the happy flow. In addition to the happy flow there is an exception flow which happens when pyodbc fails to execute the TRUNCATE TABLE sql statement. Here is a sample implementation of the unit tests that cover above scenarios. import unittest from unittest import mock from pyodbc_functions import truncate_table class Test_function_truncate_table(unittest.TestCase): def fix_dbc(self): dbc = mock.MagicMock() return dbc def test_truncate_table_calls_proper_methods_given_database_execute_is_successful(self): dbc = self.fix_dbc() truncate_table('users', dbc) with dbc.cursor() as cursor: cursor.assert_has_calls([ mock.call.execute('TRUNCATE TABLE users'), mock.call.commit() ]) def test_truncate_table_calls_rollback_on_and_propagates_exception_given_database_execute_fails(self): dbc = self.fix_dbc() with dbc.cursor() as cursor: cursor.execute.side_effect = Exception('bad boy') with self.assertRaises(Exception) as excinfo: truncate_table('users', dbc) self.assertEqual('bad boy', str(excinfo.exception)) cursor.asset_has_calls([ mock.call.execute(mock.call.ANY), mock.call.rollback() ]) We use mock database connection. In the happy flow test we pass mock database connection to the truncate_table function. Once the function is executed, we assert that following steps were made in a sequence: execute was called on the database cursor with proper SQL statement as argument commit with no arguments was called on the database cursor. In the exception flow test, we again use a mock database connection, but this time we configure the execute method of the cursor to throw an exception. We make sure the exception is propagated with assertRaises() unittest assert. We also check that the message of the exception is preserved. We verify that the flow is calling: execute method of the cursor - we do not verify the arguments since we already validated this in the happy flow. rollback method on the database connection.","title":"Testing the solution"},{"location":"python/misc/python-pyodbc-truncate-table/#discussion","text":"You can call the execute method on a connection cursor directly, but it is always better to move the code into a separate routine: provides reusability - you have a tested piece of code that can be used everywhere. improves the readability of the code - you create your own idioms or dictionary which make your code more expressive. improves the testability of the code - imagine your code truncates the table in the middle of 200+ line code fragment. How would you test it works correctly? How would you cover both scenarios? isolates your code from the external system - one of the benefits of this isolation is that you can unit test your code. You can find the pyodbc documentation here .","title":"Discussion"},{"location":"python/tdd/","text":"Python Test Driven Developmen Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Test Driven Development (TDD)"},{"location":"python/tdd/#python-test-driven-developmen","text":"Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Python Test Driven Developmen"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/","text":"Assert custom objects are equal in Python unit test Problem You are creating a Python unit test, using unittest . Your test needs to assert that two custom objects are equal. For example, you might have following User class defined: class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name Trying to compare two User objects for equality, using assertEqual fails: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') test_case.assertEqual(expected, actual) Traceback (most recent call last): ... AssertionError: <__main__.User object at 0x000002BC202AD888> != <__main__.User object at 0x000002BC2000B348> >>> Solution I will present you 3 different solutions to the problem. Implement equality interface Use addTypeEqualityFunc Use matcher Each solution is applicable to different situations. Implement equality interface Because unittest will try to perform Python's equality operator on your User objects, if the User class implements the equality operator interface, the equality assertion will work. The equality operator interface requires that the class implements a __eq__ method. class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name def __eq__(self, other): return self.id == other.id and \\ self.name == other.name Now it is ok to use assertEqual : >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(expected, actual) assertNotEqual also works fine: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'Jane') >>> test_case.assertNotEqual(expected, actual) Using this approach solves our problem, but it has some limitations: We need to modify the User class source There is no way to have different equality assertions for different situations. For example, we might have situations where equality doesn't include the id attribute. Use addTypeEqualityFunc method Unittest TestCase class provides convenient way to override default Python equality operator by using the addTypeEqualityFunc : >>> import unittest >>> test_case = unittest.TestCase() >>> test_case.addTypeEqualityFunc(User, lambda first, second, msg: first.name == second.name ) >>> expected = User(1, 'John') actual = User(2, 'John') >>> test_case.assertEqual(expected, actual) The limitations of this approach: We have to use the same comparison function for a given type in all tests. For example, we might have tests where equality check requires id fields to be equal and other tests where id fields should not be compared. Both parameters to assertEqual have to be objects of the same type. Use matcher Another approach would be to create custom matcher object. class UserMatcher: expected: User def __init__(self, expected): self.expected = expected def __eq__(self, other): return self.expected.id == other.id and \\ self.expected.name == other.name Custom matcher could be any class that implements the equality operator. In other words, having the __eq__ method implemented. >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(UserMatcher(expected), actual) Although this is the most flexible approach, it is more verbose. Discussion In case of equality assertion fails, the output is not very useful: >>> test_case.assertEqual(UserMatcher(expected), actual) Traceback (most recent call last): ... AssertionError: <__main__.UserMatcher object at 0x000002BC20309BC8> != <__main__.User object at 0x000002BC20314388> You could improve this by implementing a __repr__ method of your custom class: class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name def __repr__(self): return f\"User(id={repr(self.id)}, name={repr(self.name)})\" In case you are using matcher, the matcher should also implement the __repr__ method: class UserMatcher: expected: User def __init__(self, expected): self.expected = expected def __repr__(self): return repr(self.expected) def __eq__(self, other): return self.expected.id == other.id and \\ self.expected.name == other.name Now we will get much more readable and meaningful assertion failure message: >>> test_case.assertEqual(UserMatcher(expected), actual) ... AssertionError: User(id=1, name='John') != User(id=2, name='John')","title":"Assert custom objects are equal in Python unit test"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#assert-custom-objects-are-equal-in-python-unit-test","text":"","title":"Assert custom objects are equal in Python unit test"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#problem","text":"You are creating a Python unit test, using unittest . Your test needs to assert that two custom objects are equal. For example, you might have following User class defined: class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name Trying to compare two User objects for equality, using assertEqual fails: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') test_case.assertEqual(expected, actual) Traceback (most recent call last): ... AssertionError: <__main__.User object at 0x000002BC202AD888> != <__main__.User object at 0x000002BC2000B348> >>>","title":"Problem"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#solution","text":"I will present you 3 different solutions to the problem. Implement equality interface Use addTypeEqualityFunc Use matcher Each solution is applicable to different situations.","title":"Solution"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#implement-equality-interface","text":"Because unittest will try to perform Python's equality operator on your User objects, if the User class implements the equality operator interface, the equality assertion will work. The equality operator interface requires that the class implements a __eq__ method. class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name def __eq__(self, other): return self.id == other.id and \\ self.name == other.name Now it is ok to use assertEqual : >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(expected, actual) assertNotEqual also works fine: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'Jane') >>> test_case.assertNotEqual(expected, actual) Using this approach solves our problem, but it has some limitations: We need to modify the User class source There is no way to have different equality assertions for different situations. For example, we might have situations where equality doesn't include the id attribute.","title":"Implement equality interface"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#use-addtypeequalityfunc-method","text":"Unittest TestCase class provides convenient way to override default Python equality operator by using the addTypeEqualityFunc : >>> import unittest >>> test_case = unittest.TestCase() >>> test_case.addTypeEqualityFunc(User, lambda first, second, msg: first.name == second.name ) >>> expected = User(1, 'John') actual = User(2, 'John') >>> test_case.assertEqual(expected, actual) The limitations of this approach: We have to use the same comparison function for a given type in all tests. For example, we might have tests where equality check requires id fields to be equal and other tests where id fields should not be compared. Both parameters to assertEqual have to be objects of the same type.","title":"Use addTypeEqualityFunc method"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#use-matcher","text":"Another approach would be to create custom matcher object. class UserMatcher: expected: User def __init__(self, expected): self.expected = expected def __eq__(self, other): return self.expected.id == other.id and \\ self.expected.name == other.name Custom matcher could be any class that implements the equality operator. In other words, having the __eq__ method implemented. >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(UserMatcher(expected), actual) Although this is the most flexible approach, it is more verbose.","title":"Use matcher"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#discussion","text":"In case of equality assertion fails, the output is not very useful: >>> test_case.assertEqual(UserMatcher(expected), actual) Traceback (most recent call last): ... AssertionError: <__main__.UserMatcher object at 0x000002BC20309BC8> != <__main__.User object at 0x000002BC20314388> You could improve this by implementing a __repr__ method of your custom class: class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name def __repr__(self): return f\"User(id={repr(self.id)}, name={repr(self.name)})\" In case you are using matcher, the matcher should also implement the __repr__ method: class UserMatcher: expected: User def __init__(self, expected): self.expected = expected def __repr__(self): return repr(self.expected) def __eq__(self, other): return self.expected.id == other.id and \\ self.expected.name == other.name Now we will get much more readable and meaningful assertion failure message: >>> test_case.assertEqual(UserMatcher(expected), actual) ... AssertionError: User(id=1, name='John') != User(id=2, name='John')","title":"Discussion"},{"location":"python/tdd/python-unittest-database-applications/","text":"Unit testing for Python database applications Problem You are building an application that uses database in Python. For example, you might have created following function, which uses pyodbc to insert a list of rows into a database table. Each row is a dictionary. def insert_rows(rows, table_name, dbc): field_names = rows[0].keys() field_names_str = ', '.join(field_names) placeholder_str = ','.join('?'*len(field_names)) insert_sql = f'INSERT INTO {table_name}({field_names_str}) VALUES ({placeholder_str})' saved_autocommit = dbc.autocommit with dbc.cursor() as cursor: try: dbc.autocommit = False tuples = [ tuple((row[field_name] for field_name in field_names)) for row in rows ] cursor.executemany(insert_sql, tuples) cursor.commit() except Exception as exc: cursor.rollback() raise exc finally: dbc.autocommit = saved_autocommit How I can unit test such a function? Solution Use unittest.mock to generate mock database connection. It is as simple as: dbc = mock.MagicMock() The very first test could be to verify that our function calls the cursor() method of the database connection. import unittest from unittest import mock class Test_insert_rows(unittest.TestCase): def fix_dbc(self): dbc = mock.MagicMock(spec=['cursor']) dbc.autocommit = True return dbc def fix_rows(self): rows = [{'id':1, 'name':'John'}, {'id':2, 'name':'Jane'},] return rows def test_insert_rows_calls_cursor_method(self): dbc = self.fix_dbc() rows = self.fix_rows() insert_rows(rows, 'users', dbc) self.assertTrue(dbc.cursor.called) if __name__ == '__main__': unittest.main(argv=['', '-v']) Some highlights on what I have done in this test: The database connection, used for testing is created using a method. This is because I am gonna need this connection again and again in the test methods I am going to create. The rows fixture is also created using a method. For the same reason. At the end there is if __name__ == '__main__':... . This will run unittest if I execute the file as python script. Here is the result of the execution: (.venv37) sandbox>python test_insert_rows.py -v test_insert_rows_calls_cursor_method (test_insert_rows.Test_insert_rows) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.002s OK The test I have created so far is not very exciting. To create a better test, let's look closely at what this function does: generates insert statement generates a list of tuples from the rows list calls the executemany method of the database cursor commits the transaction So my test could verify that the function calls the executemany method with correct arguments and commits the transaction. Let's implement this test. def fix_tuples(self): tuples = [(1,'John'), (2,'Jane'),] return tuples def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments(self): dbc = self.fix_dbc() rows = self.fix_rows() insert_rows(rows, 'users', dbc) with dbc.cursor() as cursor: expect_sql = 'INSERT INTO users(id, name) VALUES (?,?)' expect_tuples = self.fix_tuples() calls = [mock.call.executemany(expect_sql, expect_tuples), mock.call.commit(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) In this test I use the assert_has_calls assertion of the mock object to verify that specific calls has been made with expected arguments and in expected order. At the end of the test I verify that autocommit property of the database connection is restored to True . Ok. Great. So far we tested the happy path. What happens if something fails? In the following test I am gonna make the database connection raise an exception to test the behavior of my function. We have to verify: Transaction is rolled back after executemany is called. Database connection autocommit property has been restored. Exception is propagated. def test_insert_rows_rollsback_transaction_on_databse_exception(self): dbc = self.fix_dbc() rows = self.fix_rows() with dbc.cursor() as cursor: cursor.executemany.side_effect = Exception('Some DB error') with self.assertRaises(Exception) as exc: insert_rows(rows, 'users', dbc) calls = [mock.call.executemany(mock.ANY, mock.ANY), mock.call.rollback(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) self.assertEqual('Some DB error', str(exc.exception)) Discussion It might seem easier using real database, but it has drawbacks: Actual database might not be available or you might not have connectivity to it. You are not testing your code in isolation. If, for example, the database connection is unstable, you will start getting wired, unpredictable results. Database requests add significant latency. You need to reset the database state before each state to a fixed well-known state. This might be challenging, especially if the database is shared. Looking at our function, we could see a smell. It does more than one thing: Generate INSERT statement Generate tuples list Insert the tuples into the database There are also some conditions that might have been handled better. What if I pass empty list of rows? The good news is, we could refactor our code and make sure our code still works properly. We know we haven't broken anything. Just because we have thorough unit tests. Refactoring - Extract function Let's move the code for generating INSERT statement into a new function. We are going to follow these steps: Run our tests and make sure they pass (green). Create a test the new function. Run our tests and make sure they fail (red). Create a new function and copy the code we want to extract. Run our tests and make sure they pass (green). Replace the old code with a call to the new function. Run our tests and make sure they still pass (green). Let's implement the test: def test_make_insert_produces_correct_statement(self): fields = ['id', 'name'] actual = make_insert_statement(fields, 'users') expected = 'INSERT INTO users(id, name) VALUES (?,?)' self.assertEqual(expected, actual) Tests are failing now: (.venv37) sandbox>python test_insert_rows.py test_insert_rows_calls_cursor_method (__main__.Test_insert_rows) ... ok test_insert_rows_calls_executemany_and_commit_passing_correct_arguments (__main__.Test_insert_rows) ... ok test_insert_rows_rollsback_transaction_on_databse_exception (__main__.Test_insert_rows) ... ok test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ... ERROR ====================================================================== ERROR: test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ---------------------------------------------------------------------- Traceback (most recent call last): File \"c:/Sandbox/Learn/Python/TestDrivenPythonDevelopment/play/sandbox/test_insert_rows.py\", line 42, in test_make_insert_produces_correct_statement actual = make_insert_statement(fields, 'users') NameError: name 'make_insert_statement' is not defined ---------------------------------------------------------------------- Ran 4 tests in 0.017s FAILED (errors=1) Now we implement the function: def make_insert_statement(field_names, table_name): field_names_str = ', '.join(field_names) placeholder_str = ','.join('?'*len(field_names)) insert_sql = f'INSERT INTO {table_name}({field_names_str}) VALUES ({placeholder_str})' return insert_sql Tests are passing. We update the insert_rows function to call the new make_insert_statement function and run the tests to see them passing. def insert_rows(rows, table_name, dbc): field_names = rows[0].keys() insert_sql = make_insert_statement(field_names, table_name) saved_autocommit = dbc.autocommit with dbc.cursor() as cursor: try: dbc.autocommit = False tuples = [ tuple((row[field_name] for field_name in field_names)) for row in rows ] cursor.executemany(insert_sql, tuples) cursor.commit() except Exception as exc: cursor.rollback() raise exc finally: dbc.autocommit = saved_autocommit The new version of the function is not much shorter, but has some advantages: improved readability - it is much easier to understand what the code is doing improved reusability - it is very likely that we might need the INSERT statement generation in another situation better testability - we could test the generation of the INSERT statement in isolation. Introducing new test cases for this functionality is easy. It doesn't require database connection, for example. If we follow the principles of the test driven development (TDD), we should remove the check for the generated statement in the call to the insertmany . We could achieve this by patching the make_insert_statement function. @mock.patch('__main__.make_insert_statement') def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments(self, make_insert_mock): dbc = self.fix_dbc() rows = self.fix_rows() make_insert_mock.return_value = 'MY PRECIOUS' insert_rows(rows, 'users', dbc) with dbc.cursor() as cursor: expect_sql = 'MY PRECIOUS' expect_tuples = self.fix_tuples() calls = [mock.call.executemany(expect_sql, expect_tuples), mock.call.commit(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) Wait! What is going here? I used the patch decorator to replace the make_insert_statement with a mock object. The mock object is automatically added as second argument to my test method. I have also defined that the make_insert_statement mock returns a fixed value MY PRECIOS . It is not valid SQL, but our mock database connection doesn't care. The important thing is that we see the result from the make_insert_statement passed to the executemany method.","title":"Unit testing for Python database applications"},{"location":"python/tdd/python-unittest-database-applications/#unit-testing-for-python-database-applications","text":"","title":"Unit testing for Python database applications"},{"location":"python/tdd/python-unittest-database-applications/#problem","text":"You are building an application that uses database in Python. For example, you might have created following function, which uses pyodbc to insert a list of rows into a database table. Each row is a dictionary. def insert_rows(rows, table_name, dbc): field_names = rows[0].keys() field_names_str = ', '.join(field_names) placeholder_str = ','.join('?'*len(field_names)) insert_sql = f'INSERT INTO {table_name}({field_names_str}) VALUES ({placeholder_str})' saved_autocommit = dbc.autocommit with dbc.cursor() as cursor: try: dbc.autocommit = False tuples = [ tuple((row[field_name] for field_name in field_names)) for row in rows ] cursor.executemany(insert_sql, tuples) cursor.commit() except Exception as exc: cursor.rollback() raise exc finally: dbc.autocommit = saved_autocommit How I can unit test such a function?","title":"Problem"},{"location":"python/tdd/python-unittest-database-applications/#solution","text":"Use unittest.mock to generate mock database connection. It is as simple as: dbc = mock.MagicMock() The very first test could be to verify that our function calls the cursor() method of the database connection. import unittest from unittest import mock class Test_insert_rows(unittest.TestCase): def fix_dbc(self): dbc = mock.MagicMock(spec=['cursor']) dbc.autocommit = True return dbc def fix_rows(self): rows = [{'id':1, 'name':'John'}, {'id':2, 'name':'Jane'},] return rows def test_insert_rows_calls_cursor_method(self): dbc = self.fix_dbc() rows = self.fix_rows() insert_rows(rows, 'users', dbc) self.assertTrue(dbc.cursor.called) if __name__ == '__main__': unittest.main(argv=['', '-v']) Some highlights on what I have done in this test: The database connection, used for testing is created using a method. This is because I am gonna need this connection again and again in the test methods I am going to create. The rows fixture is also created using a method. For the same reason. At the end there is if __name__ == '__main__':... . This will run unittest if I execute the file as python script. Here is the result of the execution: (.venv37) sandbox>python test_insert_rows.py -v test_insert_rows_calls_cursor_method (test_insert_rows.Test_insert_rows) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.002s OK The test I have created so far is not very exciting. To create a better test, let's look closely at what this function does: generates insert statement generates a list of tuples from the rows list calls the executemany method of the database cursor commits the transaction So my test could verify that the function calls the executemany method with correct arguments and commits the transaction. Let's implement this test. def fix_tuples(self): tuples = [(1,'John'), (2,'Jane'),] return tuples def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments(self): dbc = self.fix_dbc() rows = self.fix_rows() insert_rows(rows, 'users', dbc) with dbc.cursor() as cursor: expect_sql = 'INSERT INTO users(id, name) VALUES (?,?)' expect_tuples = self.fix_tuples() calls = [mock.call.executemany(expect_sql, expect_tuples), mock.call.commit(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) In this test I use the assert_has_calls assertion of the mock object to verify that specific calls has been made with expected arguments and in expected order. At the end of the test I verify that autocommit property of the database connection is restored to True . Ok. Great. So far we tested the happy path. What happens if something fails? In the following test I am gonna make the database connection raise an exception to test the behavior of my function. We have to verify: Transaction is rolled back after executemany is called. Database connection autocommit property has been restored. Exception is propagated. def test_insert_rows_rollsback_transaction_on_databse_exception(self): dbc = self.fix_dbc() rows = self.fix_rows() with dbc.cursor() as cursor: cursor.executemany.side_effect = Exception('Some DB error') with self.assertRaises(Exception) as exc: insert_rows(rows, 'users', dbc) calls = [mock.call.executemany(mock.ANY, mock.ANY), mock.call.rollback(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) self.assertEqual('Some DB error', str(exc.exception))","title":"Solution"},{"location":"python/tdd/python-unittest-database-applications/#discussion","text":"It might seem easier using real database, but it has drawbacks: Actual database might not be available or you might not have connectivity to it. You are not testing your code in isolation. If, for example, the database connection is unstable, you will start getting wired, unpredictable results. Database requests add significant latency. You need to reset the database state before each state to a fixed well-known state. This might be challenging, especially if the database is shared. Looking at our function, we could see a smell. It does more than one thing: Generate INSERT statement Generate tuples list Insert the tuples into the database There are also some conditions that might have been handled better. What if I pass empty list of rows? The good news is, we could refactor our code and make sure our code still works properly. We know we haven't broken anything. Just because we have thorough unit tests.","title":"Discussion"},{"location":"python/tdd/python-unittest-database-applications/#refactoring-extract-function","text":"Let's move the code for generating INSERT statement into a new function. We are going to follow these steps: Run our tests and make sure they pass (green). Create a test the new function. Run our tests and make sure they fail (red). Create a new function and copy the code we want to extract. Run our tests and make sure they pass (green). Replace the old code with a call to the new function. Run our tests and make sure they still pass (green). Let's implement the test: def test_make_insert_produces_correct_statement(self): fields = ['id', 'name'] actual = make_insert_statement(fields, 'users') expected = 'INSERT INTO users(id, name) VALUES (?,?)' self.assertEqual(expected, actual) Tests are failing now: (.venv37) sandbox>python test_insert_rows.py test_insert_rows_calls_cursor_method (__main__.Test_insert_rows) ... ok test_insert_rows_calls_executemany_and_commit_passing_correct_arguments (__main__.Test_insert_rows) ... ok test_insert_rows_rollsback_transaction_on_databse_exception (__main__.Test_insert_rows) ... ok test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ... ERROR ====================================================================== ERROR: test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ---------------------------------------------------------------------- Traceback (most recent call last): File \"c:/Sandbox/Learn/Python/TestDrivenPythonDevelopment/play/sandbox/test_insert_rows.py\", line 42, in test_make_insert_produces_correct_statement actual = make_insert_statement(fields, 'users') NameError: name 'make_insert_statement' is not defined ---------------------------------------------------------------------- Ran 4 tests in 0.017s FAILED (errors=1) Now we implement the function: def make_insert_statement(field_names, table_name): field_names_str = ', '.join(field_names) placeholder_str = ','.join('?'*len(field_names)) insert_sql = f'INSERT INTO {table_name}({field_names_str}) VALUES ({placeholder_str})' return insert_sql Tests are passing. We update the insert_rows function to call the new make_insert_statement function and run the tests to see them passing. def insert_rows(rows, table_name, dbc): field_names = rows[0].keys() insert_sql = make_insert_statement(field_names, table_name) saved_autocommit = dbc.autocommit with dbc.cursor() as cursor: try: dbc.autocommit = False tuples = [ tuple((row[field_name] for field_name in field_names)) for row in rows ] cursor.executemany(insert_sql, tuples) cursor.commit() except Exception as exc: cursor.rollback() raise exc finally: dbc.autocommit = saved_autocommit The new version of the function is not much shorter, but has some advantages: improved readability - it is much easier to understand what the code is doing improved reusability - it is very likely that we might need the INSERT statement generation in another situation better testability - we could test the generation of the INSERT statement in isolation. Introducing new test cases for this functionality is easy. It doesn't require database connection, for example. If we follow the principles of the test driven development (TDD), we should remove the check for the generated statement in the call to the insertmany . We could achieve this by patching the make_insert_statement function. @mock.patch('__main__.make_insert_statement') def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments(self, make_insert_mock): dbc = self.fix_dbc() rows = self.fix_rows() make_insert_mock.return_value = 'MY PRECIOUS' insert_rows(rows, 'users', dbc) with dbc.cursor() as cursor: expect_sql = 'MY PRECIOUS' expect_tuples = self.fix_tuples() calls = [mock.call.executemany(expect_sql, expect_tuples), mock.call.commit(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) Wait! What is going here? I used the patch decorator to replace the make_insert_statement with a mock object. The mock object is automatically added as second argument to my test method. I have also defined that the make_insert_statement mock returns a fixed value MY PRECIOS . It is not valid SQL, but our mock database connection doesn't care. The important thing is that we see the result from the make_insert_statement passed to the executemany method.","title":"Refactoring - Extract function"},{"location":"python/tricks/","text":"Python tricks Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks","title":"Tricks"},{"location":"python/tricks/#python-tricks","text":"Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks","title":"Python tricks"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/","text":"Unpacking a Python sequence into variables Problem You have a list, tuple or another sequence that you want to unpack into a collection of variables. Solution You use assignment operator. On the left side of the operator you define a tuple with the variables. On the right side of the operator is the sequence. The only requirement is that the number of variables and structures match the sequence. Example: >>> p = (5, 6) >>> x, y = p >>> x 5 >>> y 6 Elements of the sequence could be any objects - integer, sequence, etc. Here is an example: >>> data = [ 'ACME', 50, 91.1, (2012, 12, 21)] >>> name, shares, price, date = data >>> name 'ACME' >>> shares 50 >>> price 91.1 >>> date (2012, 12, 21) Discussion In fact unpacking works with any iterable Python object. This includes strings, files, iterators, and generators. Sometimes in your assignment, you want to ignore some of the values at certain places from the sequence. Python doesn't have a special syntax for this. Common practice is to use special variable name, e.g. underscore _ , for places you want to ignore. >>> data = [ 'ACME', 50, 91.1, (2012, 12, 21)] >>> name, _, _, date = data >>> name 'ACME' >>> date (2012, 12, 21)","title":"Unpacking a Python sequence into variables"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#unpacking-a-python-sequence-into-variables","text":"","title":"Unpacking a Python sequence into variables"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#problem","text":"You have a list, tuple or another sequence that you want to unpack into a collection of variables.","title":"Problem"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#solution","text":"You use assignment operator. On the left side of the operator you define a tuple with the variables. On the right side of the operator is the sequence. The only requirement is that the number of variables and structures match the sequence. Example: >>> p = (5, 6) >>> x, y = p >>> x 5 >>> y 6 Elements of the sequence could be any objects - integer, sequence, etc. Here is an example: >>> data = [ 'ACME', 50, 91.1, (2012, 12, 21)] >>> name, shares, price, date = data >>> name 'ACME' >>> shares 50 >>> price 91.1 >>> date (2012, 12, 21)","title":"Solution"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#discussion","text":"In fact unpacking works with any iterable Python object. This includes strings, files, iterators, and generators. Sometimes in your assignment, you want to ignore some of the values at certain places from the sequence. Python doesn't have a special syntax for this. Common practice is to use special variable name, e.g. underscore _ , for places you want to ignore. >>> data = [ 'ACME', 50, 91.1, (2012, 12, 21)] >>> name, _, _, date = data >>> name 'ACME' >>> date (2012, 12, 21)","title":"Discussion"},{"location":"sqlserver/","text":"SQL Server (MSSQL) Zone Recipes List Blocking Locks in SQL Server List roles assigned to a principal / user in SQL Server (MSSQL) Database","title":"SQL Server"},{"location":"sqlserver/#sql-server-mssql-zone","text":"","title":"SQL Server (MSSQL) Zone"},{"location":"sqlserver/#recipes","text":"List Blocking Locks in SQL Server List roles assigned to a principal / user in SQL Server (MSSQL) Database","title":"Recipes"},{"location":"sqlserver/sql-server-list-blocking-locks/","text":"List blocking locks in SQL Server (MSSQL) Database Problem You use SQL Server and you need to know which sessions are blocked and for what reason. Solution Use the following query to get a list of blocked sessions. SELECT p.cmd, p.* FROM sys.sysprocesses p WHERE blocked > 0 You can find this solution also as GitHub gist list-blocking-locks.sql .","title":"List blocking locks in SQL Server (MSSQL) Database"},{"location":"sqlserver/sql-server-list-blocking-locks/#list-blocking-locks-in-sql-server-mssql-database","text":"","title":"List blocking locks in SQL Server (MSSQL) Database"},{"location":"sqlserver/sql-server-list-blocking-locks/#problem","text":"You use SQL Server and you need to know which sessions are blocked and for what reason.","title":"Problem"},{"location":"sqlserver/sql-server-list-blocking-locks/#solution","text":"Use the following query to get a list of blocked sessions. SELECT p.cmd, p.* FROM sys.sysprocesses p WHERE blocked > 0 You can find this solution also as GitHub gist list-blocking-locks.sql .","title":"Solution"},{"location":"sqlserver/sql-server-list-find-roles-assigned-user-principalblocking-locks/","text":"List or find roles assigned to a principal / user in SQL Server (MSSQL) Database Problem You use SQL Server. You to know which roles were granted to which users (database principals). Solution To find all the role assignments to users in SQL Server database, you can use the following query. SELECT r.name role_principal_name, m.name AS member_principal_name FROM sys.database_role_members rm JOIN sys.database_principals r ON rm.role_principal_id = r.principal_id JOIN sys.database_principals m ON rm.member_principal_id = m.principal_id WHERE r.type = 'R'; You can also limit the list of roles to only the roles, assigned to a particular user or principal by adding a filtering condition to the WHERE clause. DECLARE @PrincipalName VARCHAR(128) = 'principal-name-here' SELECT r.name role_principal_name, m.name AS member_principal_name FROM sys.database_role_members rm JOIN sys.database_principals r ON rm.role_principal_id = r.principal_id JOIN sys.database_principals m ON rm.member_principal_id = m.principal_id WHERE r.type = 'R' AND m.name = @PrincipalName; You can find this solution also as GitHub gist list-principal-roles.sql .","title":"List or find roles assigned to a principal / user in SQL Server (MSSQL) Database"},{"location":"sqlserver/sql-server-list-find-roles-assigned-user-principalblocking-locks/#list-or-find-roles-assigned-to-a-principal-user-in-sql-server-mssql-database","text":"","title":"List or find roles assigned to a principal / user in SQL Server (MSSQL) Database"},{"location":"sqlserver/sql-server-list-find-roles-assigned-user-principalblocking-locks/#problem","text":"You use SQL Server. You to know which roles were granted to which users (database principals).","title":"Problem"},{"location":"sqlserver/sql-server-list-find-roles-assigned-user-principalblocking-locks/#solution","text":"To find all the role assignments to users in SQL Server database, you can use the following query. SELECT r.name role_principal_name, m.name AS member_principal_name FROM sys.database_role_members rm JOIN sys.database_principals r ON rm.role_principal_id = r.principal_id JOIN sys.database_principals m ON rm.member_principal_id = m.principal_id WHERE r.type = 'R'; You can also limit the list of roles to only the roles, assigned to a particular user or principal by adding a filtering condition to the WHERE clause. DECLARE @PrincipalName VARCHAR(128) = 'principal-name-here' SELECT r.name role_principal_name, m.name AS member_principal_name FROM sys.database_role_members rm JOIN sys.database_principals r ON rm.role_principal_id = r.principal_id JOIN sys.database_principals m ON rm.member_principal_id = m.principal_id WHERE r.type = 'R' AND m.name = @PrincipalName; You can find this solution also as GitHub gist list-principal-roles.sql .","title":"Solution"}]}