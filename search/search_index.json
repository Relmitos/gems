{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my Gems Latest Articles Python Kata #1: Pipe and Filter on August 22th, 2020, in Python :: Kata Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks Truncate table with pyodbc on August 17th, 2020, in Python :: Misc Topics Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Home"},{"location":"#welcome-to-my-gems","text":"","title":"Welcome to my Gems"},{"location":"#latest-articles","text":"Python Kata #1: Pipe and Filter on August 22th, 2020, in Python :: Kata Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks Truncate table with pyodbc on August 17th, 2020, in Python :: Misc Topics Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Latest Articles"},{"location":"about/","text":"Clamantia vixque accipit tandem quodque penetrale frigida Some page Tolle prope Lorem markdownum, mihi est et quater gradibus albam dabat Cinyras, moderator duas. Potest spem hamos an sanguine cultu, capit vate habeo oris radix: pastor traduxit. Iacentes messes est surgere nunc lacrimas nutrix illic; matris sollertia subire: non etiam iter dixerat foret dixit. Nexibus pars aranea iungit te abiit exosus ipse: sole non ait tuis interius illa. Domum in manus. Hamis valuere ea Paucaque tamquam nexu, et inquit prisca indicium obruerat dici, novat castris ne visus et meae pharetra legit, dea. Fortius amore. Animalia commota: faces quod nova, Sigei rudibusque quorum. Esset tu sperato famulosne laterum dixit et tecto argenteus prece. Nunc exacta esse esset, adolescere vices si inde. Vestigia cruoris. Per sinistra passa: nil aequorea contiguas ecquis maduere Ossaque , amico est sic, magnum. Novissima ab ferunt concipit sinuantur ramum, dum secundas sedant. Hic populo vides, sic vidit exprimitur gloria cum ecce volucres vulnere pastoris! Nec vosque Iovem atque , veli quam deorum sacra esset est flere nimium. Lingua pectora validoque adnuit et rogat Cacuminat litus; vixque Idan paulum inseritur sacris, inde locuta segnior. Sacris Lyrnesia parte, illi vigebat Munychiosque Iuno vacuas, qui sine novis rima chelydri, verba. Fuissem retegatur cursu studiisque fatalia femina, crepitantia circumspexit lucos Diamque pavere, qui. Genitor somnus Apollinis aditus Doleam quae criminis, murra an putas et quoque , simulatas ipse volucres, dum Phlegethontide . Credi et sorori fertur: subitam, sed summae deperit promissa polumque. Convaluit aurum, amicitur miseri ante , sum fugae roboris exspectant bisque membra. Amori sic ut deterrere unde ferrumque! Volenti bis ars recondere simul montis Spercheides agmine parenti! Ostentis Athamas ut referam Pyrrha, sanguis ibi invenit saxa. Neptunus et alter blanditias venenifero fulget: una silvas creditur; luctu sceleratae septem ita simulac stantis resonare? Invitaque domos.","title":"Clamantia vixque accipit tandem quodque penetrale frigida"},{"location":"about/#clamantia-vixque-accipit-tandem-quodque-penetrale-frigida","text":"Some page","title":"Clamantia vixque accipit tandem quodque penetrale frigida"},{"location":"about/#tolle-prope","text":"Lorem markdownum, mihi est et quater gradibus albam dabat Cinyras, moderator duas. Potest spem hamos an sanguine cultu, capit vate habeo oris radix: pastor traduxit. Iacentes messes est surgere nunc lacrimas nutrix illic; matris sollertia subire: non etiam iter dixerat foret dixit. Nexibus pars aranea iungit te abiit exosus ipse: sole non ait tuis interius illa. Domum in manus.","title":"Tolle prope"},{"location":"about/#hamis-valuere-ea","text":"Paucaque tamquam nexu, et inquit prisca indicium obruerat dici, novat castris ne visus et meae pharetra legit, dea. Fortius amore. Animalia commota: faces quod nova, Sigei rudibusque quorum. Esset tu sperato famulosne laterum dixit et tecto argenteus prece. Nunc exacta esse esset, adolescere vices si inde. Vestigia cruoris. Per sinistra passa: nil aequorea contiguas ecquis maduere Ossaque , amico est sic, magnum. Novissima ab ferunt concipit sinuantur ramum, dum secundas sedant. Hic populo vides, sic vidit exprimitur gloria cum ecce volucres vulnere pastoris! Nec vosque Iovem atque , veli quam deorum sacra esset est flere nimium.","title":"Hamis valuere ea"},{"location":"about/#lingua-pectora-validoque-adnuit-et-rogat","text":"Cacuminat litus; vixque Idan paulum inseritur sacris, inde locuta segnior. Sacris Lyrnesia parte, illi vigebat Munychiosque Iuno vacuas, qui sine novis rima chelydri, verba. Fuissem retegatur cursu studiisque fatalia femina, crepitantia circumspexit lucos Diamque pavere, qui.","title":"Lingua pectora validoque adnuit et rogat"},{"location":"about/#genitor-somnus-apollinis-aditus","text":"Doleam quae criminis, murra an putas et quoque , simulatas ipse volucres, dum Phlegethontide . Credi et sorori fertur: subitam, sed summae deperit promissa polumque. Convaluit aurum, amicitur miseri ante , sum fugae roboris exspectant bisque membra. Amori sic ut deterrere unde ferrumque! Volenti bis ars recondere simul montis Spercheides agmine parenti! Ostentis Athamas ut referam Pyrrha, sanguis ibi invenit saxa. Neptunus et alter blanditias venenifero fulget: una silvas creditur; luctu sceleratae septem ita simulac stantis resonare? Invitaque domos.","title":"Genitor somnus Apollinis aditus"},{"location":"page/","text":"Hello","title":"Hello"},{"location":"page/#hello","text":"","title":"Hello"},{"location":"blog/","text":"Blog Created article: Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks Cleared AZ-900 on August 17th, 2020 Created this site on August 15th, 2020 Created article: Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Created article: Unit testing for Python database applications on August 15th, 2020, in Python :: Test Driven Development Cleared AZ-400 exam with Microsoft Certified: DevOps Engineer Expert certificate on August 10th, 2020, in Blog Cleared AZ-103 exam with Microsoft Certified: Azure Administrator Associate certificate on June 2nd, 2020, in Blog","title":"Blog"},{"location":"blog/#blog","text":"Created article: Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks Cleared AZ-900 on August 17th, 2020 Created this site on August 15th, 2020 Created article: Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Created article: Unit testing for Python database applications on August 15th, 2020, in Python :: Test Driven Development Cleared AZ-400 exam with Microsoft Certified: DevOps Engineer Expert certificate on August 10th, 2020, in Blog Cleared AZ-103 exam with Microsoft Certified: Azure Administrator Associate certificate on June 2nd, 2020, in Blog","title":"Blog"},{"location":"blog/2020-06-02-cleared-az-103/","text":"Cleared AZ-103 exam Today I cleared the AZ-103 exam : Microsoft Azure Administrator Achieved certificate: Microsoft Certified: Azure Administrator Associate","title":"Cleared AZ-103 exam"},{"location":"blog/2020-06-02-cleared-az-103/#cleared-az-103-exam","text":"Today I cleared the AZ-103 exam : Microsoft Azure Administrator Achieved certificate: Microsoft Certified: Azure Administrator Associate","title":"Cleared AZ-103 exam"},{"location":"blog/2020-08-10-cleared-az-400/","text":"Cleared AZ-400 exam Today I cleared the AZ-400 exam : Designing and Implementing Microsoft DevOps Solutions Together with earlier Microsoft Certified Azure Administrator Associate certificate, I got the Microsoft Certified DevOps Engineer Expert .","title":"Cleared AZ-400 exam"},{"location":"blog/2020-08-10-cleared-az-400/#cleared-az-400-exam","text":"Today I cleared the AZ-400 exam : Designing and Implementing Microsoft DevOps Solutions Together with earlier Microsoft Certified Azure Administrator Associate certificate, I got the Microsoft Certified DevOps Engineer Expert .","title":"Cleared AZ-400 exam"},{"location":"python/","text":"Python Test Driven Development in Python (TDD) Python Kata Python Tricks Misc Python topics","title":"Python"},{"location":"python/#python","text":"Test Driven Development in Python (TDD) Python Kata Python Tricks Misc Python topics","title":"Python"},{"location":"python/kata/","text":"Python Kata Kata #1: Pipe and Filter in Python","title":"Kata"},{"location":"python/kata/#python-kata","text":"Kata #1: Pipe and Filter in Python","title":"Python Kata"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/","text":"Iterate over hierarchical sources in Python Problem Scenario We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = ['a.txt', 'b.txt'] for file_name in file_list: with open(file_name, 'r') as file: for line in file.readline(): for word in line.split(' '): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() Wow. Not very easy to read already, but it serves the purpose. How about testing? You think - it is pretty straightforward. \"I did couple of exploration tests during development. I am pretty confident it works!\" Well ... wait to see what comes next. Day 1. New requirements: Stop words should be inserted into a table stop_words . Rest of the words are still going into the words table. Day 2. New requirements: Our database administrator has some concerns about the performance and wants that the commit statement happens after 1000 words has been inserted into each table. This means you keep track on number of inserted words in each table separately. When any of the table reaches 1000 you commit inserts for that table and reset the counter for that table. Day 3. New requirement: We should be able to extract words from Word files. Bug: words are not split on punctuation. Bug: multiple spaces cause empty words to be inserted. New requirement: words need to be stored in lower case. Week 5. You worked very hard, but you are still unable to finish the Word files processing. Your manager asks you to deliver without this feature. She also assigns three new developers to your project as new requirements arrive. You need to be able to process PDF files Week 25. Everything works more or less fine. Now the privacy team comes into the picture. Only approved files should be processed. You receive a list of approved files in a CSV file. You need to be able to OCR images You need to create a build pipeline for continuous integration The solution needs to implement unit tests with at least 70% coverage. Unit tests should be executed at build time. Year 1. Privacy team has provided REST API to validate the file eligibility for processing. Can you remember how the whole this hell thing works? Real-time processing! List of files is stored in a database table. It is updated constantly. Our solution needs to process each new file entry as soon as possible. We want that words are inserted not only into the database, but also sent to a message queue. What if? What if our processing looks like this: for word in extract_words(): process_word(word) Even better: db_processor = word_process.get_database_processor() extractor = word_extract.get() reader.when_new_word(processor.process) Solution Let's start with the original requirements. Can we make the code more readable","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#iterate-over-hierarchical-sources-in-python","text":"","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#problem","text":"","title":"Problem"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#scenario","text":"We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = ['a.txt', 'b.txt'] for file_name in file_list: with open(file_name, 'r') as file: for line in file.readline(): for word in line.split(' '): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() Wow. Not very easy to read already, but it serves the purpose. How about testing? You think - it is pretty straightforward. \"I did couple of exploration tests during development. I am pretty confident it works!\" Well ... wait to see what comes next. Day 1. New requirements: Stop words should be inserted into a table stop_words . Rest of the words are still going into the words table. Day 2. New requirements: Our database administrator has some concerns about the performance and wants that the commit statement happens after 1000 words has been inserted into each table. This means you keep track on number of inserted words in each table separately. When any of the table reaches 1000 you commit inserts for that table and reset the counter for that table. Day 3. New requirement: We should be able to extract words from Word files. Bug: words are not split on punctuation. Bug: multiple spaces cause empty words to be inserted. New requirement: words need to be stored in lower case. Week 5. You worked very hard, but you are still unable to finish the Word files processing. Your manager asks you to deliver without this feature. She also assigns three new developers to your project as new requirements arrive. You need to be able to process PDF files Week 25. Everything works more or less fine. Now the privacy team comes into the picture. Only approved files should be processed. You receive a list of approved files in a CSV file. You need to be able to OCR images You need to create a build pipeline for continuous integration The solution needs to implement unit tests with at least 70% coverage. Unit tests should be executed at build time. Year 1. Privacy team has provided REST API to validate the file eligibility for processing. Can you remember how the whole this hell thing works? Real-time processing! List of files is stored in a database table. It is updated constantly. Our solution needs to process each new file entry as soon as possible. We want that words are inserted not only into the database, but also sent to a message queue.","title":"Scenario"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#what-if","text":"What if our processing looks like this: for word in extract_words(): process_word(word) Even better: db_processor = word_process.get_database_processor() extractor = word_extract.get() reader.when_new_word(processor.process)","title":"What if?"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources%20%28long%20draft%29/#solution","text":"Let's start with the original requirements. Can we make the code more readable","title":"Solution"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources/","text":"Iterate over hierarchical sources in Python Problem We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = ['a.txt', 'b.txt'] for file_name in file_list: with open(file_name, 'r') as file: for line in file: for word in line.split(' '): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() How do you test such code? Maybe you use something I call 'evolutionary testing' - you start with the outer loop. Add a print statement, run a few times, comment the print statement. And you continue until it is done. Ok. This might work at the beginning. But how you test your code continuously? There are situations where you need to verify the code is working correctly. For example: New requirements - e.g. process PDF, image files etc. Dependency upgrade - e.g. changing to a higher version of Python Change the target database Issue fixing Error handling This code is already difficult to read and understand. You want to improve your code - make it testable, more readable and maintainable, more flexible and extensible. Discussion Look at the code. Isn't it doing too much things? Solution Let's start with the responsibilities. Iterate over a list of files Process a file - iterate over the lines from a file Get the lines from a file Process a line - iterate over the words in a line Get the words from a line Process a word - save a word into the database So we might come with an object-oriented solution like this: class WordLoader: def _get_line_words(self, line): return line.split(' ') def _get_file_lines(file_name): with open(file_name, 'r') as file: return f def _process_word(self, word): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() def _process_line(self, line): for word in self._get_line_words(line): self._process_word(word) def _process_file(self, file_name): for line in self._get_file_lines(file_name): self._process_line(line) def __call__(self, file_list): for file_name in file_list: self._process_file(file_name) Looks much better now. We can test each method in isolation with unit tests. However I still have some concerns. The WordLoader class we created takes too much responsibilities. What is going on is still not very visible. To understand what is going on, I need to dig into the whole chain of execution. Can I make my code more expressive so that I can read the code from the very beginning like this: \"Insert all the words from a list of files into the database.\" Maybe my main code could look like this? for word in words_from_files: save_word(word) Where the words_from_files comes from? files = get_files_from_name_list(file_list) lines_from_files = get_lines_from_files(files) words_from_files = get_words_from_lines(lines_from_files) Wait! You are going to load all these terabytes and petabytes into the memory? Good point. Not necessarily. Instead of returning list, I could use generators. def get_files_from_name_list(filename_list): for filename in filename_list: with open(filename, 'r') as file: yield file def get_lines_from_files(files): for file in files: for line in file: yield file def get_words_from_lines(lines): for line in lines: for word in line.split(' ') yield word The code is much more expressive now. There are two things I do not like: functions look almost the same functions are doing more than one thing. Let's take get_lines_from_files as an example. It iterates over the files in a list, unpacks the file into lines, using split() method and iterates over the resulting words. def unpack_containers(containers, unpack): for container in containers: for item in unpack(container): yield item def unpack_filename(filename): with open(filename, 'r') as f: yield [f] def unpack_file_lines(file): return file def unpack_line_words(line): for word in line.split(' '): yeild word files = unpack_containers(file_list, lambda filename: [open(filename, 'r')]) lines_from_files = get_lines_from_files(files) words_from_files = get_words_from_lines(lines_from_files) files = map(open_file, file_list) lines = chain.from_iterable(files) words = chain.from_iterable(map(lambda line: line.split(' '), lines)) def open_file(filename): with open(filename, 'r') as f: yield f def get_words_from_line(line): return line.split(' ') def flat_map(function, iterable, *arg): arg.insert(0, iterable) chain.from_iterable(map(function, *arg)) lines = map(open_file, file_list) words = flat_map(get_words_from_line, lines) def list_new_events(): with db.cursor() as c: c.execute(\"SELECT * FROM events WHERE status='N'\") yield c.fetchall() def repeat(function, before=None, after=None): while True: if before is not None: before() yield function() if after is not None: after() def reset_process(): pass def process(event): pass event_stream = chain.from_iterable(repeat, after=reset_process) for event in event_stream: process(event)","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources/#iterate-over-hierarchical-sources-in-python","text":"","title":"Iterate over hierarchical sources in Python"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources/#problem","text":"We are given a list of files and we need to insert the words from files into a database. The sequence of the words should be preserved. Here is one way to achieve this: file_list = ['a.txt', 'b.txt'] for file_name in file_list: with open(file_name, 'r') as file: for line in file: for word in line.split(' '): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() How do you test such code? Maybe you use something I call 'evolutionary testing' - you start with the outer loop. Add a print statement, run a few times, comment the print statement. And you continue until it is done. Ok. This might work at the beginning. But how you test your code continuously? There are situations where you need to verify the code is working correctly. For example: New requirements - e.g. process PDF, image files etc. Dependency upgrade - e.g. changing to a higher version of Python Change the target database Issue fixing Error handling This code is already difficult to read and understand. You want to improve your code - make it testable, more readable and maintainable, more flexible and extensible.","title":"Problem"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources/#discussion","text":"Look at the code. Isn't it doing too much things?","title":"Discussion"},{"location":"python/kata/python-kata-iterate-over-hierarchical-sources/#solution","text":"Let's start with the responsibilities. Iterate over a list of files Process a file - iterate over the lines from a file Get the lines from a file Process a line - iterate over the words in a line Get the words from a line Process a word - save a word into the database So we might come with an object-oriented solution like this: class WordLoader: def _get_line_words(self, line): return line.split(' ') def _get_file_lines(file_name): with open(file_name, 'r') as file: return f def _process_word(self, word): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() def _process_line(self, line): for word in self._get_line_words(line): self._process_word(word) def _process_file(self, file_name): for line in self._get_file_lines(file_name): self._process_line(line) def __call__(self, file_list): for file_name in file_list: self._process_file(file_name) Looks much better now. We can test each method in isolation with unit tests. However I still have some concerns. The WordLoader class we created takes too much responsibilities. What is going on is still not very visible. To understand what is going on, I need to dig into the whole chain of execution. Can I make my code more expressive so that I can read the code from the very beginning like this: \"Insert all the words from a list of files into the database.\" Maybe my main code could look like this? for word in words_from_files: save_word(word) Where the words_from_files comes from? files = get_files_from_name_list(file_list) lines_from_files = get_lines_from_files(files) words_from_files = get_words_from_lines(lines_from_files) Wait! You are going to load all these terabytes and petabytes into the memory? Good point. Not necessarily. Instead of returning list, I could use generators. def get_files_from_name_list(filename_list): for filename in filename_list: with open(filename, 'r') as file: yield file def get_lines_from_files(files): for file in files: for line in file: yield file def get_words_from_lines(lines): for line in lines: for word in line.split(' ') yield word The code is much more expressive now. There are two things I do not like: functions look almost the same functions are doing more than one thing. Let's take get_lines_from_files as an example. It iterates over the files in a list, unpacks the file into lines, using split() method and iterates over the resulting words. def unpack_containers(containers, unpack): for container in containers: for item in unpack(container): yield item def unpack_filename(filename): with open(filename, 'r') as f: yield [f] def unpack_file_lines(file): return file def unpack_line_words(line): for word in line.split(' '): yeild word files = unpack_containers(file_list, lambda filename: [open(filename, 'r')]) lines_from_files = get_lines_from_files(files) words_from_files = get_words_from_lines(lines_from_files) files = map(open_file, file_list) lines = chain.from_iterable(files) words = chain.from_iterable(map(lambda line: line.split(' '), lines)) def open_file(filename): with open(filename, 'r') as f: yield f def get_words_from_line(line): return line.split(' ') def flat_map(function, iterable, *arg): arg.insert(0, iterable) chain.from_iterable(map(function, *arg)) lines = map(open_file, file_list) words = flat_map(get_words_from_line, lines) def list_new_events(): with db.cursor() as c: c.execute(\"SELECT * FROM events WHERE status='N'\") yield c.fetchall() def repeat(function, before=None, after=None): while True: if before is not None: before() yield function() if after is not None: after() def reset_process(): pass def process(event): pass event_stream = chain.from_iterable(repeat, after=reset_process) for event in event_stream: process(event)","title":"Solution"},{"location":"python/kata/python-kata-pipe-and-filter-solution/","text":"Pipe and Filter in Python The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. Here is an example what it might look like using the implementation. Example: Input: a list of integers [4,6,9] Transformation 1: divide each integer by 2 and convert to integer Transformation 2: convert each item into a sequence, e.g. 1 becomes [1], 2 becomes [2, 2], 3 becomes [3,3,3], etc. Flatten the result. Python code: pipe = (Pipe([4,6,9]) .map(lambda x: x/2) .flat_map(lambda x: str(x)*int(x))) for item in pipe: print(item) Output: 2 2 3 3 3 4 4 4 4","title":"Pipe and Filter in Python"},{"location":"python/kata/python-kata-pipe-and-filter-solution/#pipe-and-filter-in-python","text":"The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. Here is an example what it might look like using the implementation. Example: Input: a list of integers [4,6,9] Transformation 1: divide each integer by 2 and convert to integer Transformation 2: convert each item into a sequence, e.g. 1 becomes [1], 2 becomes [2, 2], 3 becomes [3,3,3], etc. Flatten the result. Python code: pipe = (Pipe([4,6,9]) .map(lambda x: x/2) .flat_map(lambda x: str(x)*int(x))) for item in pipe: print(item) Output: 2 2 3 3 3 4 4 4 4","title":"Pipe and Filter in Python"},{"location":"python/kata/python-kata-pipe-and-filter/","text":"Python Kata #1: Pipe and Filter The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. filter - takes as an argument a bool function. Function is applied for each element in the input. If the filter function returns true, the element is placed in the output. Otherwise the element is dropped. Do not forget to test your solution! Here is an example what it might look like using the implementation. Example: Input: a list of strings ['Lorem ipsum', 'dolorem costum'] Transformation 1: convert upper case characters to lower case: ['lorem ipsum', 'dolorem costum'] Transformation 2: split each element into: ['lorem', 'ipsum', 'dolorem', 'costum'] Transformation 3: remove all words ending with 'em': ['ipsum', 'costum'] Python code: pipe = (Pipe(['Lorem ipsum', 'dolorem costum']) .map(lambda x: x.lower()) .flat_map(lambda x: x.split(' ')) .filter(lambda x: not x.endswith('em'))) for item in pipe: print(item) Output: ipsum costum","title":"Python Kata #1: Pipe and Filter"},{"location":"python/kata/python-kata-pipe-and-filter/#python-kata-1-pipe-and-filter","text":"The Pipe and Filter is a very popular architecture style. It can be found in may software development frameworks, like Spark, JavaScript, etc. Linux command line shell also supports piping. You can run one command and direct the output of that command to another command. The output of the second command can be directed to a third command and so on and so forth. We want to have something similar in Python - piping or chaining together series of transformations on a sequence. Transformations are applied with following patterns: map - takes as an argument a transformation. When the pipeline is executed, the transformation is applied to each element from the input and is placed in the output. flat_map - takes as an argument a transformation. The transformation is expected to produce iterable. When the pipeline is executed, the transformation is applied to each element from the input, each element from the resulting iterable is placed into the output pipeline. filter - takes as an argument a bool function. Function is applied for each element in the input. If the filter function returns true, the element is placed in the output. Otherwise the element is dropped. Do not forget to test your solution! Here is an example what it might look like using the implementation. Example: Input: a list of strings ['Lorem ipsum', 'dolorem costum'] Transformation 1: convert upper case characters to lower case: ['lorem ipsum', 'dolorem costum'] Transformation 2: split each element into: ['lorem', 'ipsum', 'dolorem', 'costum'] Transformation 3: remove all words ending with 'em': ['ipsum', 'costum'] Python code: pipe = (Pipe(['Lorem ipsum', 'dolorem costum']) .map(lambda x: x.lower()) .flat_map(lambda x: x.split(' ')) .filter(lambda x: not x.endswith('em'))) for item in pipe: print(item) Output: ipsum costum","title":"Python Kata #1: Pipe and Filter"},{"location":"python/kata/python-kata-public-transportation-solution/","text":"Public Transportation - Python Kata Solution Solution 1 Your solution might look like the following: import os STAGING_DIR = '.' while True: for filename in os.listdir(STAGING_DIR): filepath = os.path.join(STAGING_DIR, filename) if os.path.isfile(filepath): with open(filepath, 'r') as file: for line in file: for word in line.split(' '): save_word(word) Although this is not the worst solution I have seen, it has some weaknesses: Deep statement nesting at 6 nesting levels. This makes the code very difficult. Where should I add the sleep calls? No error handling. If you add try-except blocks, nesting will become even deeper. Difficult to test. You might have followed the \"growing onion layers\" approach which helped in the initial development, but how would you test changes, bug fixes, Python version upgrades? Difficult to reuse. Actually templating or as I prefer calling it - copy/paste/edit - is the only way to reuse such a code. Solution 2 class WordLoader: def _get_line_words(self, line): return line.split(' ') def _get_file_lines(file_name): with open(file_name, 'r') as file: yield file def _process_word(self, word): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() def _process_line(self, line): for word in self._get_line_words(line): self._process_word(word) def _process_file(self, file_name): for line in self._get_file_lines(file_name): self._process_line(line) def __call__(self, file_list): for file_name in file_list: self._process_file(file_name) load_file_list = FileLoader() while True: file_list = [f for f in os.listdir(STAGING_DIR) if os.path.isfile(os.path.join(STAGING_DIR, f))] load_file_list(file_list) Solution 3 def repeat(function, before=None, after=None): while True: try: value = function() except StopIteration: break if before is not None: before(value) yield value if after is not None: after(value) def pipeline(iterable, function, before=None, after=None): for item in terable: if before is not None: item = before(item) function(item) if after is not None: after(item) def ls_staged_files(): file_list = [f for f in os.listdir(STAGING_DIR) if \\ os.path.isfile(os.path.join(STAGING_DIR, f))] return file_list def after_file_list(item): time.sleep(3) def get_lines_from_files(files): for filename in files: filepath = os.path.join(STAGING_DIR, filename) with open(filepath, 'r') as file: for line in file: yield line def get_words_from_lines(lines): for line in lines: for word in line.split(' '): yield word def open_files(filenames): for filename in filenames: filepath = os.path.join(STAGING_DIR, filename) with open(filepath, 'r') as file: yield file STAGING_DIR = '.' filelist_sequence = repeat(ls_staged_files, after=after_file_list) filename_sequence = itertools.chain.from_iterable(filelist_sequence) file_sequence = open_files(filename_sequence) file_sequence = itertools.chain.from_iterable(repeat(ls_staged_files, after=after_file_list)) line_sequence = get_lines_from_files(file_sequence) word_sequence = get_words_from_lines(line_sequence) for word in word_sequence: print(word)","title":"Public Transportation - Python Kata Solution"},{"location":"python/kata/python-kata-public-transportation-solution/#public-transportation-python-kata-solution","text":"","title":"Public Transportation - Python Kata Solution"},{"location":"python/kata/python-kata-public-transportation-solution/#solution-1","text":"Your solution might look like the following: import os STAGING_DIR = '.' while True: for filename in os.listdir(STAGING_DIR): filepath = os.path.join(STAGING_DIR, filename) if os.path.isfile(filepath): with open(filepath, 'r') as file: for line in file: for word in line.split(' '): save_word(word) Although this is not the worst solution I have seen, it has some weaknesses: Deep statement nesting at 6 nesting levels. This makes the code very difficult. Where should I add the sleep calls? No error handling. If you add try-except blocks, nesting will become even deeper. Difficult to test. You might have followed the \"growing onion layers\" approach which helped in the initial development, but how would you test changes, bug fixes, Python version upgrades? Difficult to reuse. Actually templating or as I prefer calling it - copy/paste/edit - is the only way to reuse such a code.","title":"Solution 1"},{"location":"python/kata/python-kata-public-transportation-solution/#solution-2","text":"class WordLoader: def _get_line_words(self, line): return line.split(' ') def _get_file_lines(file_name): with open(file_name, 'r') as file: yield file def _process_word(self, word): with db.connection.cursor() as cursor: cursor.execute('INSERT INTO words(word) VALUES (?)', (word,)) cursor.commit() def _process_line(self, line): for word in self._get_line_words(line): self._process_word(word) def _process_file(self, file_name): for line in self._get_file_lines(file_name): self._process_line(line) def __call__(self, file_list): for file_name in file_list: self._process_file(file_name) load_file_list = FileLoader() while True: file_list = [f for f in os.listdir(STAGING_DIR) if os.path.isfile(os.path.join(STAGING_DIR, f))] load_file_list(file_list)","title":"Solution 2"},{"location":"python/kata/python-kata-public-transportation-solution/#solution-3","text":"def repeat(function, before=None, after=None): while True: try: value = function() except StopIteration: break if before is not None: before(value) yield value if after is not None: after(value) def pipeline(iterable, function, before=None, after=None): for item in terable: if before is not None: item = before(item) function(item) if after is not None: after(item) def ls_staged_files(): file_list = [f for f in os.listdir(STAGING_DIR) if \\ os.path.isfile(os.path.join(STAGING_DIR, f))] return file_list def after_file_list(item): time.sleep(3) def get_lines_from_files(files): for filename in files: filepath = os.path.join(STAGING_DIR, filename) with open(filepath, 'r') as file: for line in file: yield line def get_words_from_lines(lines): for line in lines: for word in line.split(' '): yield word def open_files(filenames): for filename in filenames: filepath = os.path.join(STAGING_DIR, filename) with open(filepath, 'r') as file: yield file STAGING_DIR = '.' filelist_sequence = repeat(ls_staged_files, after=after_file_list) filename_sequence = itertools.chain.from_iterable(filelist_sequence) file_sequence = open_files(filename_sequence) file_sequence = itertools.chain.from_iterable(repeat(ls_staged_files, after=after_file_list)) line_sequence = get_lines_from_files(file_sequence) word_sequence = get_words_from_lines(line_sequence) for word in word_sequence: print(word)","title":"Solution 3"},{"location":"python/kata/python-kata-public-transportation/","text":"Public Transportation - Python Kata Imagine you live in Amsterdam and you want to travel to Paris. Of course you want to travel as soon as possible, but you have to take in account the railways timetable. Railways work in batch mode. They come on a schedule, pick everybody from the train station and move them. In our software development practice we face similar problems every day. Here is a similar one. Problem Files are being delivered by multiple source systems into a staging folder. These files are our passengers. You need to create a solution which periodically is taking a list of available files and is processing them by extracting the words from the files and inserting into a database table. This is processing solution is our train. In fact what we are creating is a pipeline. The pipeline is using batches - the list of available files. Think about following: How do you test your solution? How do you handle exceptions? How easy is for other people (or you after a year) to read and understand your solution? How would you add pre- and post-processing of words to your pipeline? For example, as pre-processing you might need to convert the word to lower case, strip punctuation characters. Post-processing might be to sleep for 2 seconds. How would you add pre- and post-processing of batches to your pipeline? For example, as post-processing, you might want to sleep for 20 seconds. How would you reuse this solution in other scenarios? For example - your staging is a database table and as processing an email is sent.","title":"Public Transportation - Python Kata"},{"location":"python/kata/python-kata-public-transportation/#public-transportation-python-kata","text":"Imagine you live in Amsterdam and you want to travel to Paris. Of course you want to travel as soon as possible, but you have to take in account the railways timetable. Railways work in batch mode. They come on a schedule, pick everybody from the train station and move them. In our software development practice we face similar problems every day. Here is a similar one.","title":"Public Transportation - Python Kata"},{"location":"python/kata/python-kata-public-transportation/#problem","text":"Files are being delivered by multiple source systems into a staging folder. These files are our passengers. You need to create a solution which periodically is taking a list of available files and is processing them by extracting the words from the files and inserting into a database table. This is processing solution is our train. In fact what we are creating is a pipeline. The pipeline is using batches - the list of available files. Think about following: How do you test your solution? How do you handle exceptions? How easy is for other people (or you after a year) to read and understand your solution? How would you add pre- and post-processing of words to your pipeline? For example, as pre-processing you might need to convert the word to lower case, strip punctuation characters. Post-processing might be to sleep for 2 seconds. How would you add pre- and post-processing of batches to your pipeline? For example, as post-processing, you might want to sleep for 20 seconds. How would you reuse this solution in other scenarios? For example - your staging is a database table and as processing an email is sent.","title":"Problem"},{"location":"python/misc/","text":"Misc Python Topics Truncate table with pyodbc on August 17th, 2020, in Python :: Misc Topics","title":"Misc Python Topics"},{"location":"python/misc/#misc-python-topics","text":"Truncate table with pyodbc on August 17th, 2020, in Python :: Misc Topics","title":"Misc Python Topics"},{"location":"python/misc/python-pyodbc-truncate-table/","text":"Truncate table with pyodbc Problem You need to truncate a table using pyodbc . Solution Here is an example of a function to truncate a database table, using pyodbc connection. You can find the full source in GitHub . def truncate_table(table_ref, dbc): try: with dbc.cursor() as cursor: cursor.execute(f'TRUNCATE TABLE {table_ref}') cursor.commit() except Exception as err: dbc.rollback() raise err Testing the solution Although the function is simple, it needs testing. The function should perform two steps: Truncate the table, executing TRUNCATE TABLE sql statement Commit the transaction This is the happy flow. In addition to the happy flow there is an exception flow which happens when pyodbc fails to execute the TRUNCATE TABLE sql statement. Here is a sample implementation of the unit tests that cover above scenarios. import unittest from unittest import mock from pyodbc_functions import truncate_table class Test_function_truncate_table(unittest.TestCase): def fix_dbc(self): dbc = mock.MagicMock() return dbc def test_truncate_table_calls_proper_methods_given_database_execute_is_successful(self): dbc = self.fix_dbc() truncate_table('users', dbc) with dbc.cursor() as cursor: cursor.assert_has_calls([ mock.call.execute('TRUNCATE TABLE users'), mock.call.commit() ]) def test_truncate_table_calls_rollback_on_and_propagates_exception_given_database_execute_fails(self): dbc = self.fix_dbc() with dbc.cursor() as cursor: cursor.execute.side_effect = Exception('bad boy') with self.assertRaises(Exception) as excinfo: truncate_table('users', dbc) self.assertEqual('bad boy', str(excinfo.exception)) cursor.asset_has_calls([ mock.call.execute(mock.call.ANY), mock.call.rollback() ]) We use mock database connection. In the happy flow test we pass mock database connection to the truncate_table function. Once the function is executed, we assert that following steps were made in a sequence: execute was called on the database cursor with proper SQL statement as argument commit with no arguments was called on the database cursor. In the exception flow test, we again use a mock database connection, but this time we configure the execute method of the cursor to throw an exception. We make sure the exception is propagated with assertRaises() unittest assert. We also check that the message of the exception is preserved. We verify that the flow is calling: execute method of the cursor - we do not verify the arguments since we already validated this in the happy flow. rollback method on the database connection. Discussion You can call the execute method on a connection cursor directly, but it is always better to move the code into a separate routine: provides reusability - you have a tested piece of code that can be used everywhere. improves the readability of the code - you create your own idioms or dictionary which make your code more expressive. improves the testability of the code - imagine your code truncates the table in the middle of 200+ line code fragment. How would you test it works correctly? How would you cover both scenarios? isolates your code from the external system - one of the benefits of this isolation is that you can unit test your code. You can find the pyodbc documentation here .","title":"Truncate table with pyodbc"},{"location":"python/misc/python-pyodbc-truncate-table/#truncate-table-with-pyodbc","text":"","title":"Truncate table with pyodbc"},{"location":"python/misc/python-pyodbc-truncate-table/#problem","text":"You need to truncate a table using pyodbc .","title":"Problem"},{"location":"python/misc/python-pyodbc-truncate-table/#solution","text":"Here is an example of a function to truncate a database table, using pyodbc connection. You can find the full source in GitHub . def truncate_table(table_ref, dbc): try: with dbc.cursor() as cursor: cursor.execute(f'TRUNCATE TABLE {table_ref}') cursor.commit() except Exception as err: dbc.rollback() raise err","title":"Solution"},{"location":"python/misc/python-pyodbc-truncate-table/#testing-the-solution","text":"Although the function is simple, it needs testing. The function should perform two steps: Truncate the table, executing TRUNCATE TABLE sql statement Commit the transaction This is the happy flow. In addition to the happy flow there is an exception flow which happens when pyodbc fails to execute the TRUNCATE TABLE sql statement. Here is a sample implementation of the unit tests that cover above scenarios. import unittest from unittest import mock from pyodbc_functions import truncate_table class Test_function_truncate_table(unittest.TestCase): def fix_dbc(self): dbc = mock.MagicMock() return dbc def test_truncate_table_calls_proper_methods_given_database_execute_is_successful(self): dbc = self.fix_dbc() truncate_table('users', dbc) with dbc.cursor() as cursor: cursor.assert_has_calls([ mock.call.execute('TRUNCATE TABLE users'), mock.call.commit() ]) def test_truncate_table_calls_rollback_on_and_propagates_exception_given_database_execute_fails(self): dbc = self.fix_dbc() with dbc.cursor() as cursor: cursor.execute.side_effect = Exception('bad boy') with self.assertRaises(Exception) as excinfo: truncate_table('users', dbc) self.assertEqual('bad boy', str(excinfo.exception)) cursor.asset_has_calls([ mock.call.execute(mock.call.ANY), mock.call.rollback() ]) We use mock database connection. In the happy flow test we pass mock database connection to the truncate_table function. Once the function is executed, we assert that following steps were made in a sequence: execute was called on the database cursor with proper SQL statement as argument commit with no arguments was called on the database cursor. In the exception flow test, we again use a mock database connection, but this time we configure the execute method of the cursor to throw an exception. We make sure the exception is propagated with assertRaises() unittest assert. We also check that the message of the exception is preserved. We verify that the flow is calling: execute method of the cursor - we do not verify the arguments since we already validated this in the happy flow. rollback method on the database connection.","title":"Testing the solution"},{"location":"python/misc/python-pyodbc-truncate-table/#discussion","text":"You can call the execute method on a connection cursor directly, but it is always better to move the code into a separate routine: provides reusability - you have a tested piece of code that can be used everywhere. improves the readability of the code - you create your own idioms or dictionary which make your code more expressive. improves the testability of the code - imagine your code truncates the table in the middle of 200+ line code fragment. How would you test it works correctly? How would you cover both scenarios? isolates your code from the external system - one of the benefits of this isolation is that you can unit test your code. You can find the pyodbc documentation here .","title":"Discussion"},{"location":"python/tdd/","text":"Python Test Driven Developmen Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Test Driven Development (TDD)"},{"location":"python/tdd/#python-test-driven-developmen","text":"Assert custom objects are equal in Python unit test on August 15th, 2020, in Python :: Test Driven Development Unit testing for Python database applications on August 14th, 2020, in Python :: Test Driven Development","title":"Python Test Driven Developmen"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/","text":"Assert custom objects are equal in Python unit test Problem You are creating a Python unit test, using unittest . Your test needs to assert that two custom objects are equal. For example, you might have following User class defined: class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name Trying to compare two User objects for equality, using assertEqual fails: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') test_case.assertEqual(expected, actual) Traceback (most recent call last): ... AssertionError: <__main__.User object at 0x000002BC202AD888> != <__main__.User object at 0x000002BC2000B348> >>> Solution I will present you 3 different solutions to the problem. Implement equality interface Use addTypeEqualityFunc Use matcher Each solution is applicable to different situations. Implement equality interface Because unittest will try to perform Python's equality operator on your User objects, if the User class implements the equality operator interface, the equality assertion will work. The equality operator interface requires that the class implements a __eq__ method. class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name def __eq__(self, other): return self.id == other.id and \\ self.name == other.name Now it is ok to use assertEqual : >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(expected, actual) assertNotEqual also works fine: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'Jane') >>> test_case.assertNotEqual(expected, actual) Using this approach solves our problem, but it has some limitations: We need to modify the User class source There is no way to have different equality assertions for different situations. For example, we might have situations where equality doesn't include the id attribute. Use addTypeEqualityFunc method Unittest TestCase class provides convenient way to override default Python equality operator by using the addTypeEqualityFunc : >>> import unittest >>> test_case = unittest.TestCase() >>> test_case.addTypeEqualityFunc(User, lambda first, second, msg: first.name == second.name ) >>> expected = User(1, 'John') actual = User(2, 'John') >>> test_case.assertEqual(expected, actual) The limitations of this approach: We have to use the same comparison function for a given type in all tests. For example, we might have tests where equality check requires id fields to be equal and other tests where id fields should not be compared. Both parameters to assertEqual have to be objects of the same type. Use matcher Another approach would be to create custom matcher object. class UserMatcher: expected: User def __init__(self, expected): self.expected = expected def __eq__(self, other): return self.expected.id == other.id and \\ self.expected.name == other.name Custom matcher could be any class that implements the equality operator. In other words, having the __eq__ method implemented. >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(UserMatcher(expected), actual) Although this is the most flexible approach, it is more verbose. Discussion In case of equality assertion fails, the output is not very useful: >>> test_case.assertEqual(UserMatcher(expected), actual) Traceback (most recent call last): ... AssertionError: <__main__.UserMatcher object at 0x000002BC20309BC8> != <__main__.User object at 0x000002BC20314388> You could improve this by implementing a __repr__ method of your custom class: class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name def __repr__(self): return f\"User(id={repr(self.id)}, name={repr(self.name)})\" In case you are using matcher, the matcher should also implement the __repr__ method: class UserMatcher: expected: User def __init__(self, expected): self.expected = expected def __repr__(self): return repr(self.expected) def __eq__(self, other): return self.expected.id == other.id and \\ self.expected.name == other.name Now we will get much more readable and meaningful assertion failure message: >>> test_case.assertEqual(UserMatcher(expected), actual) ... AssertionError: User(id=1, name='John') != User(id=2, name='John')","title":"Assert custom objects are equal in Python unit test"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#assert-custom-objects-are-equal-in-python-unit-test","text":"","title":"Assert custom objects are equal in Python unit test"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#problem","text":"You are creating a Python unit test, using unittest . Your test needs to assert that two custom objects are equal. For example, you might have following User class defined: class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name Trying to compare two User objects for equality, using assertEqual fails: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') test_case.assertEqual(expected, actual) Traceback (most recent call last): ... AssertionError: <__main__.User object at 0x000002BC202AD888> != <__main__.User object at 0x000002BC2000B348> >>>","title":"Problem"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#solution","text":"I will present you 3 different solutions to the problem. Implement equality interface Use addTypeEqualityFunc Use matcher Each solution is applicable to different situations.","title":"Solution"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#implement-equality-interface","text":"Because unittest will try to perform Python's equality operator on your User objects, if the User class implements the equality operator interface, the equality assertion will work. The equality operator interface requires that the class implements a __eq__ method. class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name def __eq__(self, other): return self.id == other.id and \\ self.name == other.name Now it is ok to use assertEqual : >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(expected, actual) assertNotEqual also works fine: >>> import unittest >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'Jane') >>> test_case.assertNotEqual(expected, actual) Using this approach solves our problem, but it has some limitations: We need to modify the User class source There is no way to have different equality assertions for different situations. For example, we might have situations where equality doesn't include the id attribute.","title":"Implement equality interface"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#use-addtypeequalityfunc-method","text":"Unittest TestCase class provides convenient way to override default Python equality operator by using the addTypeEqualityFunc : >>> import unittest >>> test_case = unittest.TestCase() >>> test_case.addTypeEqualityFunc(User, lambda first, second, msg: first.name == second.name ) >>> expected = User(1, 'John') actual = User(2, 'John') >>> test_case.assertEqual(expected, actual) The limitations of this approach: We have to use the same comparison function for a given type in all tests. For example, we might have tests where equality check requires id fields to be equal and other tests where id fields should not be compared. Both parameters to assertEqual have to be objects of the same type.","title":"Use addTypeEqualityFunc method"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#use-matcher","text":"Another approach would be to create custom matcher object. class UserMatcher: expected: User def __init__(self, expected): self.expected = expected def __eq__(self, other): return self.expected.id == other.id and \\ self.expected.name == other.name Custom matcher could be any class that implements the equality operator. In other words, having the __eq__ method implemented. >>> test_case = unittest.TestCase() >>> expected = User(1, 'John') >>> actual = User(1, 'John') >>> test_case.assertEqual(UserMatcher(expected), actual) Although this is the most flexible approach, it is more verbose.","title":"Use matcher"},{"location":"python/tdd/python-unittest-assert-custom-objects-are-equal/#discussion","text":"In case of equality assertion fails, the output is not very useful: >>> test_case.assertEqual(UserMatcher(expected), actual) Traceback (most recent call last): ... AssertionError: <__main__.UserMatcher object at 0x000002BC20309BC8> != <__main__.User object at 0x000002BC20314388> You could improve this by implementing a __repr__ method of your custom class: class User: id: int name: str def __init__(self, id, name): self.id = id self.name = name def __repr__(self): return f\"User(id={repr(self.id)}, name={repr(self.name)})\" In case you are using matcher, the matcher should also implement the __repr__ method: class UserMatcher: expected: User def __init__(self, expected): self.expected = expected def __repr__(self): return repr(self.expected) def __eq__(self, other): return self.expected.id == other.id and \\ self.expected.name == other.name Now we will get much more readable and meaningful assertion failure message: >>> test_case.assertEqual(UserMatcher(expected), actual) ... AssertionError: User(id=1, name='John') != User(id=2, name='John')","title":"Discussion"},{"location":"python/tdd/python-unittest-database-applications/","text":"Unit testing for Python database applications Problem You are building an application that uses database in Python. For example, you might have created following function, which uses pyodbc to insert a list of rows into a database table. Each row is a dictionary. def insert_rows(rows, table_name, dbc): field_names = rows[0].keys() field_names_str = ', '.join(field_names) placeholder_str = ','.join('?'*len(field_names)) insert_sql = f'INSERT INTO {table_name}({field_names_str}) VALUES ({placeholder_str})' saved_autocommit = dbc.autocommit with dbc.cursor() as cursor: try: dbc.autocommit = False tuples = [ tuple((row[field_name] for field_name in field_names)) for row in rows ] cursor.executemany(insert_sql, tuples) cursor.commit() except Exception as exc: cursor.rollback() raise exc finally: dbc.autocommit = saved_autocommit How I can unit test such a function? Solution Use unittest.mock to generate mock database connection. It is as simple as: dbc = mock.MagicMock() The very first test could be to verify that our function calls the cursor() method of the database connection. import unittest from unittest import mock class Test_insert_rows(unittest.TestCase): def fix_dbc(self): dbc = mock.MagicMock(spec=['cursor']) dbc.autocommit = True return dbc def fix_rows(self): rows = [{'id':1, 'name':'John'}, {'id':2, 'name':'Jane'},] return rows def test_insert_rows_calls_cursor_method(self): dbc = self.fix_dbc() rows = self.fix_rows() insert_rows(rows, 'users', dbc) self.assertTrue(dbc.cursor.called) if __name__ == '__main__': unittest.main(argv=['', '-v']) Some highlights on what I have done in this test: The database connection, used for testing is created using a method. This is because I am gonna need this connection again and again in the test methods I am going to create. The rows fixture is also created using a method. For the same reason. At the end there is if __name__ == '__main__':... . This will run unittest if I execute the file as python script. Here is the result of the execution: (.venv37) sandbox>python test_insert_rows.py -v test_insert_rows_calls_cursor_method (test_insert_rows.Test_insert_rows) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.002s OK The test I have created so far is not very exciting. To create a better test, let's look closely at what this function does: generates insert statement generates a list of tuples from the rows list calls the executemany method of the database cursor commits the transaction So my test could verify that the function calls the executemany method with correct arguments and commits the transaction. Let's implement this test. def fix_tuples(self): tuples = [(1,'John'), (2,'Jane'),] return tuples def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments(self): dbc = self.fix_dbc() rows = self.fix_rows() insert_rows(rows, 'users', dbc) with dbc.cursor() as cursor: expect_sql = 'INSERT INTO users(id, name) VALUES (?,?)' expect_tuples = self.fix_tuples() calls = [mock.call.executemany(expect_sql, expect_tuples), mock.call.commit(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) In this test I use the assert_has_calls assertion of the mock object to verify that specific calls has been made with expected arguments and in expected order. At the end of the test I verify that autocommit property of the database connection is restored to True . Ok. Great. So far we tested the happy path. What happens if something fails? In the following test I am gonna make the database connection raise an exception to test the behavior of my function. We have to verify: Transaction is rolled back after executemany is called. Database connection autocommit property has been restored. Exception is propagated. def test_insert_rows_rollsback_transaction_on_databse_exception(self): dbc = self.fix_dbc() rows = self.fix_rows() with dbc.cursor() as cursor: cursor.executemany.side_effect = Exception('Some DB error') with self.assertRaises(Exception) as exc: insert_rows(rows, 'users', dbc) calls = [mock.call.executemany(mock.ANY, mock.ANY), mock.call.rollback(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) self.assertEqual('Some DB error', str(exc.exception)) Discussion It might seem easier using real database, but it has drawbacks: Actual database might not be available or you might not have connectivity to it. You are not testing your code in isolation. If, for example, the database connection is unstable, you will start getting wired, unpredictable results. Database requests add significant latency. You need to reset the database state before each state to a fixed well-known state. This might be challenging, especially if the database is shared. Looking at our function, we could see a smell. It does more than one thing: Generate INSERT statement Generate tuples list Insert the tuples into the database There are also some conditions that might have been handled better. What if I pass empty list of rows? The good news is, we could refactor our code and make sure our code still works properly. We know we haven't broken anything. Just because we have thorough unit tests. Refactoring - Extract function Let's move the code for generating INSERT statement into a new function. We are going to follow these steps: Run our tests and make sure they pass (green). Create a test the new function. Run our tests and make sure they fail (red). Create a new function and copy the code we want to extract. Run our tests and make sure they pass (green). Replace the old code with a call to the new function. Run our tests and make sure they still pass (green). Let's implement the test: def test_make_insert_produces_correct_statement(self): fields = ['id', 'name'] actual = make_insert_statement(fields, 'users') expected = 'INSERT INTO users(id, name) VALUES (?,?)' self.assertEqual(expected, actual) Tests are failing now: (.venv37) sandbox>python test_insert_rows.py test_insert_rows_calls_cursor_method (__main__.Test_insert_rows) ... ok test_insert_rows_calls_executemany_and_commit_passing_correct_arguments (__main__.Test_insert_rows) ... ok test_insert_rows_rollsback_transaction_on_databse_exception (__main__.Test_insert_rows) ... ok test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ... ERROR ====================================================================== ERROR: test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ---------------------------------------------------------------------- Traceback (most recent call last): File \"c:/Sandbox/Learn/Python/TestDrivenPythonDevelopment/play/sandbox/test_insert_rows.py\", line 42, in test_make_insert_produces_correct_statement actual = make_insert_statement(fields, 'users') NameError: name 'make_insert_statement' is not defined ---------------------------------------------------------------------- Ran 4 tests in 0.017s FAILED (errors=1) Now we implement the function: def make_insert_statement(field_names, table_name): field_names_str = ', '.join(field_names) placeholder_str = ','.join('?'*len(field_names)) insert_sql = f'INSERT INTO {table_name}({field_names_str}) VALUES ({placeholder_str})' return insert_sql Tests are passing. We update the insert_rows function to call the new make_insert_statement function and run the tests to see them passing. def insert_rows(rows, table_name, dbc): field_names = rows[0].keys() insert_sql = make_insert_statement(field_names, table_name) saved_autocommit = dbc.autocommit with dbc.cursor() as cursor: try: dbc.autocommit = False tuples = [ tuple((row[field_name] for field_name in field_names)) for row in rows ] cursor.executemany(insert_sql, tuples) cursor.commit() except Exception as exc: cursor.rollback() raise exc finally: dbc.autocommit = saved_autocommit The new version of the function is not much shorter, but has some advantages: improved readability - it is much easier to understand what the code is doing improved reusability - it is very likely that we might need the INSERT statement generation in another situation better testability - we could test the generation of the INSERT statement in isolation. Introducing new test cases for this functionality is easy. It doesn't require database connection, for example. If we follow the principles of the test driven development (TDD), we should remove the check for the generated statement in the call to the insertmany . We could achieve this by patching the make_insert_statement function. @mock.patch('__main__.make_insert_statement') def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments(self, make_insert_mock): dbc = self.fix_dbc() rows = self.fix_rows() make_insert_mock.return_value = 'MY PRECIOUS' insert_rows(rows, 'users', dbc) with dbc.cursor() as cursor: expect_sql = 'MY PRECIOUS' expect_tuples = self.fix_tuples() calls = [mock.call.executemany(expect_sql, expect_tuples), mock.call.commit(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) Wait! What is going here? I used the patch decorator to replace the make_insert_statement with a mock object. The mock object is automatically added as second argument to my test method. I have also defined that the make_insert_statement mock returns a fixed value MY PRECIOS . It is not valid SQL, but our mock database connection doesn't care. The important thing is that we see the result from the make_insert_statement passed to the executemany method.","title":"Unit testing for Python database applications"},{"location":"python/tdd/python-unittest-database-applications/#unit-testing-for-python-database-applications","text":"","title":"Unit testing for Python database applications"},{"location":"python/tdd/python-unittest-database-applications/#problem","text":"You are building an application that uses database in Python. For example, you might have created following function, which uses pyodbc to insert a list of rows into a database table. Each row is a dictionary. def insert_rows(rows, table_name, dbc): field_names = rows[0].keys() field_names_str = ', '.join(field_names) placeholder_str = ','.join('?'*len(field_names)) insert_sql = f'INSERT INTO {table_name}({field_names_str}) VALUES ({placeholder_str})' saved_autocommit = dbc.autocommit with dbc.cursor() as cursor: try: dbc.autocommit = False tuples = [ tuple((row[field_name] for field_name in field_names)) for row in rows ] cursor.executemany(insert_sql, tuples) cursor.commit() except Exception as exc: cursor.rollback() raise exc finally: dbc.autocommit = saved_autocommit How I can unit test such a function?","title":"Problem"},{"location":"python/tdd/python-unittest-database-applications/#solution","text":"Use unittest.mock to generate mock database connection. It is as simple as: dbc = mock.MagicMock() The very first test could be to verify that our function calls the cursor() method of the database connection. import unittest from unittest import mock class Test_insert_rows(unittest.TestCase): def fix_dbc(self): dbc = mock.MagicMock(spec=['cursor']) dbc.autocommit = True return dbc def fix_rows(self): rows = [{'id':1, 'name':'John'}, {'id':2, 'name':'Jane'},] return rows def test_insert_rows_calls_cursor_method(self): dbc = self.fix_dbc() rows = self.fix_rows() insert_rows(rows, 'users', dbc) self.assertTrue(dbc.cursor.called) if __name__ == '__main__': unittest.main(argv=['', '-v']) Some highlights on what I have done in this test: The database connection, used for testing is created using a method. This is because I am gonna need this connection again and again in the test methods I am going to create. The rows fixture is also created using a method. For the same reason. At the end there is if __name__ == '__main__':... . This will run unittest if I execute the file as python script. Here is the result of the execution: (.venv37) sandbox>python test_insert_rows.py -v test_insert_rows_calls_cursor_method (test_insert_rows.Test_insert_rows) ... ok ---------------------------------------------------------------------- Ran 1 test in 0.002s OK The test I have created so far is not very exciting. To create a better test, let's look closely at what this function does: generates insert statement generates a list of tuples from the rows list calls the executemany method of the database cursor commits the transaction So my test could verify that the function calls the executemany method with correct arguments and commits the transaction. Let's implement this test. def fix_tuples(self): tuples = [(1,'John'), (2,'Jane'),] return tuples def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments(self): dbc = self.fix_dbc() rows = self.fix_rows() insert_rows(rows, 'users', dbc) with dbc.cursor() as cursor: expect_sql = 'INSERT INTO users(id, name) VALUES (?,?)' expect_tuples = self.fix_tuples() calls = [mock.call.executemany(expect_sql, expect_tuples), mock.call.commit(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) In this test I use the assert_has_calls assertion of the mock object to verify that specific calls has been made with expected arguments and in expected order. At the end of the test I verify that autocommit property of the database connection is restored to True . Ok. Great. So far we tested the happy path. What happens if something fails? In the following test I am gonna make the database connection raise an exception to test the behavior of my function. We have to verify: Transaction is rolled back after executemany is called. Database connection autocommit property has been restored. Exception is propagated. def test_insert_rows_rollsback_transaction_on_databse_exception(self): dbc = self.fix_dbc() rows = self.fix_rows() with dbc.cursor() as cursor: cursor.executemany.side_effect = Exception('Some DB error') with self.assertRaises(Exception) as exc: insert_rows(rows, 'users', dbc) calls = [mock.call.executemany(mock.ANY, mock.ANY), mock.call.rollback(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) self.assertEqual('Some DB error', str(exc.exception))","title":"Solution"},{"location":"python/tdd/python-unittest-database-applications/#discussion","text":"It might seem easier using real database, but it has drawbacks: Actual database might not be available or you might not have connectivity to it. You are not testing your code in isolation. If, for example, the database connection is unstable, you will start getting wired, unpredictable results. Database requests add significant latency. You need to reset the database state before each state to a fixed well-known state. This might be challenging, especially if the database is shared. Looking at our function, we could see a smell. It does more than one thing: Generate INSERT statement Generate tuples list Insert the tuples into the database There are also some conditions that might have been handled better. What if I pass empty list of rows? The good news is, we could refactor our code and make sure our code still works properly. We know we haven't broken anything. Just because we have thorough unit tests.","title":"Discussion"},{"location":"python/tdd/python-unittest-database-applications/#refactoring-extract-function","text":"Let's move the code for generating INSERT statement into a new function. We are going to follow these steps: Run our tests and make sure they pass (green). Create a test the new function. Run our tests and make sure they fail (red). Create a new function and copy the code we want to extract. Run our tests and make sure they pass (green). Replace the old code with a call to the new function. Run our tests and make sure they still pass (green). Let's implement the test: def test_make_insert_produces_correct_statement(self): fields = ['id', 'name'] actual = make_insert_statement(fields, 'users') expected = 'INSERT INTO users(id, name) VALUES (?,?)' self.assertEqual(expected, actual) Tests are failing now: (.venv37) sandbox>python test_insert_rows.py test_insert_rows_calls_cursor_method (__main__.Test_insert_rows) ... ok test_insert_rows_calls_executemany_and_commit_passing_correct_arguments (__main__.Test_insert_rows) ... ok test_insert_rows_rollsback_transaction_on_databse_exception (__main__.Test_insert_rows) ... ok test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ... ERROR ====================================================================== ERROR: test_make_insert_produces_correct_statement (__main__.Test_insert_rows) ---------------------------------------------------------------------- Traceback (most recent call last): File \"c:/Sandbox/Learn/Python/TestDrivenPythonDevelopment/play/sandbox/test_insert_rows.py\", line 42, in test_make_insert_produces_correct_statement actual = make_insert_statement(fields, 'users') NameError: name 'make_insert_statement' is not defined ---------------------------------------------------------------------- Ran 4 tests in 0.017s FAILED (errors=1) Now we implement the function: def make_insert_statement(field_names, table_name): field_names_str = ', '.join(field_names) placeholder_str = ','.join('?'*len(field_names)) insert_sql = f'INSERT INTO {table_name}({field_names_str}) VALUES ({placeholder_str})' return insert_sql Tests are passing. We update the insert_rows function to call the new make_insert_statement function and run the tests to see them passing. def insert_rows(rows, table_name, dbc): field_names = rows[0].keys() insert_sql = make_insert_statement(field_names, table_name) saved_autocommit = dbc.autocommit with dbc.cursor() as cursor: try: dbc.autocommit = False tuples = [ tuple((row[field_name] for field_name in field_names)) for row in rows ] cursor.executemany(insert_sql, tuples) cursor.commit() except Exception as exc: cursor.rollback() raise exc finally: dbc.autocommit = saved_autocommit The new version of the function is not much shorter, but has some advantages: improved readability - it is much easier to understand what the code is doing improved reusability - it is very likely that we might need the INSERT statement generation in another situation better testability - we could test the generation of the INSERT statement in isolation. Introducing new test cases for this functionality is easy. It doesn't require database connection, for example. If we follow the principles of the test driven development (TDD), we should remove the check for the generated statement in the call to the insertmany . We could achieve this by patching the make_insert_statement function. @mock.patch('__main__.make_insert_statement') def test_insert_rows_calls_executemany_and_commit_passing_correct_arguments(self, make_insert_mock): dbc = self.fix_dbc() rows = self.fix_rows() make_insert_mock.return_value = 'MY PRECIOUS' insert_rows(rows, 'users', dbc) with dbc.cursor() as cursor: expect_sql = 'MY PRECIOUS' expect_tuples = self.fix_tuples() calls = [mock.call.executemany(expect_sql, expect_tuples), mock.call.commit(),] cursor.assert_has_calls(calls) self.assertTrue(dbc.autocommit) Wait! What is going here? I used the patch decorator to replace the make_insert_statement with a mock object. The mock object is automatically added as second argument to my test method. I have also defined that the make_insert_statement mock returns a fixed value MY PRECIOS . It is not valid SQL, but our mock database connection doesn't care. The important thing is that we see the result from the make_insert_statement passed to the executemany method.","title":"Refactoring - Extract function"},{"location":"python/tricks/","text":"Python tricks Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks","title":"Tricks"},{"location":"python/tricks/#python-tricks","text":"Unpacking a Python sequence into variables on August 19th, 2020, in Python :: Tricks","title":"Python tricks"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/","text":"Unpacking a Python sequence into variables Problem You have a list, tuple or another sequence that you want to unpack into a collection of variables. Solution You use assignment operator. On the left side of the operator you define a tuple with the variables. On the right side of the operator is the sequence. The only requirement is that the number of variables and structures match the sequence. Example: >>> p = (5, 6) >>> x, y = p >>> x 5 >>> y 6 Elements of the sequence could be any objects - integer, sequence, etc. Here is an example: >>> data = [ 'ACME', 50, 91.1, (2012, 12, 21)] >>> name, shares, price, date = data >>> name 'ACME' >>> shares 50 >>> price 91.1 >>> date (2012, 12, 21) Discussion In fact unpacking works with any iterable Python object. This includes strings, files, iterators, and generators. Sometimes in your assignment, you want to ignore some of the values at certain places from the sequence. Python doesn't have a special syntax for this. Common practice is to use special variable name, e.g. underscore _ , for places you want to ignore. >>> data = [ 'ACME', 50, 91.1, (2012, 12, 21)] >>> name, _, _, date = data >>> name 'ACME' >>> date (2012, 12, 21)","title":"Unpacking a Python sequence into variables"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#unpacking-a-python-sequence-into-variables","text":"","title":"Unpacking a Python sequence into variables"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#problem","text":"You have a list, tuple or another sequence that you want to unpack into a collection of variables.","title":"Problem"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#solution","text":"You use assignment operator. On the left side of the operator you define a tuple with the variables. On the right side of the operator is the sequence. The only requirement is that the number of variables and structures match the sequence. Example: >>> p = (5, 6) >>> x, y = p >>> x 5 >>> y 6 Elements of the sequence could be any objects - integer, sequence, etc. Here is an example: >>> data = [ 'ACME', 50, 91.1, (2012, 12, 21)] >>> name, shares, price, date = data >>> name 'ACME' >>> shares 50 >>> price 91.1 >>> date (2012, 12, 21)","title":"Solution"},{"location":"python/tricks/python-trick-unpack-a-sequence-into-variables/#discussion","text":"In fact unpacking works with any iterable Python object. This includes strings, files, iterators, and generators. Sometimes in your assignment, you want to ignore some of the values at certain places from the sequence. Python doesn't have a special syntax for this. Common practice is to use special variable name, e.g. underscore _ , for places you want to ignore. >>> data = [ 'ACME', 50, 91.1, (2012, 12, 21)] >>> name, _, _, date = data >>> name 'ACME' >>> date (2012, 12, 21)","title":"Discussion"}]}